{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse, parse_tree\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, GRU, LSTM, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)\n",
    "\n",
    "MIN_WORD_FREQUENCY = 2\n",
    "SEQUENCE_LEN = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача II : Генерация текстов\n",
    "На лекции по рекуррентным нейронным сетям рассматривалась модель генерации текста с использованием символьной языковой модели.\n",
    "Вам необходимо написать нейронную модель, предназначенную для генерации текста, но чтобы как базовые единицы использовались слова (а не символы).\n",
    "\n",
    "После чего, необходимо подсчитать перплексию для языковой модели (https://en.wikipedia.org/wiki/Perplexity). Помните, что перплексия рассчитывается на новых текстах. \n",
    "То есть Вы возьмете какую-либо книгу (в электронном формате), и обучите на ней языковую модель. Затем возьмите другое произведение этого же автора и вычислите на ней перплексию.\n",
    "\n",
    "Задача предполагает использование рекуррентной нейронной сети как основы, но с точки зрения архитектуры Вы свободны в плане выбора. Примените как LSTM, так и GRU и сравните их перплексию (и общий взгляд на адекватность генерируемого текста).\n",
    "Вы можете добавить какие угодно (рабочие) модификации архитектуры, и описать их в сравнительном анализе, что даст дополнительные баллы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Имена файлов с тренировочным и тестовым наборами:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"olesya.txt_Ascii.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 145247\n",
      "Corpus length in words: 23911\n"
     ]
    }
   ],
   "source": [
    "with open(corpus) as f:\n",
    "    text = f.read().lower().replace('\\n', ' \\n ')\n",
    "print('Corpus length in characters:', len(text))\n",
    "\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_in_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = \" \".join(doc)\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мой', 'слуга', 'повар', 'и', 'спутник', 'по', 'охоте', 'полесовщик', 'ярмола', 'вошел', 'в', 'комнату', 'согнувшись', 'под', 'вязанкой', 'дров', 'сбросил', 'ее', 'с', 'грохотом', 'на', 'пол', 'и', 'подышал', 'на', 'замерзшие', 'пальцы', 'у', 'какой', 'ветер', 'паныч', 'на', 'дворе', 'сказал', 'он', 'садясь', 'на', 'корточки', 'перед', 'заслонкой', 'нужно', 'хорошо', 'в', 'грубке', 'протопить', 'позвольте', 'запалочку', 'паныч', 'значит', 'завтра', 'на', 'зайцев', 'не', 'пойдем', 'а', 'как', 'ты', 'думаешь', 'ярмола', 'нет', 'не', 'можно', 'слышите', 'какая', 'завируха', 'заяц', 'теперь', 'лежит', 'и', 'а', 'ни', 'мурмур', 'завтра', 'и', 'одного', 'следа', 'не', 'увидите', 'судьба', 'забросила', 'меня', 'на', 'целых', 'шесть', 'месяцев', 'в', 'глухую', 'деревушку', 'волынской', 'губернии', 'на', 'окраину', 'полесья', 'и', 'охота', 'была', 'единственным', 'моим', 'занятием', 'и', 'удовольствием', 'признаюсь', 'в', 'то', 'время', 'когда', 'мне', 'предложили', 'ехать', 'в', 'деревню', 'я', 'вовсе', 'не', 'думал', 'так', 'нестерпимо', 'скучать', 'я', 'поехал', 'даже', 'с', 'радостью', 'полесье', 'глушь', 'лоно', 'природы', 'простые', 'нравы', 'первобытные', 'натуры', 'думал', 'я', 'сидя', 'в', 'вагоне', 'совсем', 'незнакомый', 'мне', 'народ', 'со', 'странными', 'обычаями', 'своеобразным', 'языком', 'и', 'уж', 'наверно', 'какое', 'множество', 'поэтических', 'легенд', 'преданий', 'и', 'песен', 'а', 'я', 'в', 'то', 'время', 'рассказывать', 'так', 'все', 'рассказывать', 'уж', 'успел', 'тиснуть', 'в', 'одной', 'маленькой', 'газетке', 'рассказ', 'с', 'двумя', 'убийствами', 'и', 'одним', 'самоубийством', 'и', 'знал', 'теоретически', 'что', 'для', 'писателей', 'полезно', 'наблюдать', 'нравы', 'но', 'или', 'перебродские', 'крестьяне', 'отличались', 'какоюто', 'особенной', 'упорной', 'несообщительностью', 'или', 'я', 'не', 'умел']\n",
      "Total Tokens: 22122\n",
      "Unique Tokens: 7305\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "text_in_words = clean_doc(text_in_words)\n",
    "print(text_in_words[:200])\n",
    "print('Total Tokens: %d' % len(text_in_words))\n",
    "print('Unique Tokens: %d' % len(set(text_in_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Посчитаем некоторые статистики*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 7305\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 2083\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно наблюдать, что большая часть слов встречается в тексте всего 1 раз, но если мы удалим их из словаря, то получим неполноценный авторский словарь и бессмысленные предложения. Предлагается так же использовать последовательности длиной 2, чтобы сохранить большую часть авторских слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 16782\n",
      "Remaining sequences: 5335\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "STEP = 1\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored+1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['пол', 'если', 'в', 'такую', 'минуту'],\n",
       " ['ко', 'мне', 'свое', 'лицо', 'в'],\n",
       " ['моих', 'слов', 'иногда', 'мне', 'казалось'],\n",
       " ['слов', 'иногда', 'мне', 'казалось', 'что'],\n",
       " ['в', 'ней', 'всего', 'лишь', 'несколько']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[3000:3005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, labels_original, percentage_test=10):\n",
    "    print('Shuffling sentences')\n",
    "    tmp_sentences = []\n",
    "    tmp_next_char = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_char.append(labels_original[i])\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_char[:cut_index], tmp_next_char[cut_index:]\n",
    "\n",
    "    print(\"Training set = %d\\nTest set = %d\" % (len(x_train), len(y_test)))\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Training set = 4320\n",
      "Test set = 481\n"
     ]
    }
   ],
   "source": [
    "sentences, next_words, sentences_test, next_words_test = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Генератор для fit_generator():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используемые архитектуры:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(SEQUENCE_LEN, len(words))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GRU():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=(SEQUENCE_LEN, len(words))))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./checkpoints/LSTM_text-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(words),\n",
    "    SEQUENCE_LEN,\n",
    "    MIN_WORD_FREQUENCY\n",
    ")\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 6.6342 - acc: 0.0365 - val_loss: 6.3559 - val_acc: 0.0527\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 6.0079 - acc: 0.0441 - val_loss: 6.4668 - val_acc: 0.0527\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 5.6160 - acc: 0.0579 - val_loss: 7.0288 - val_acc: 0.0527\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 5.1661 - acc: 0.0919 - val_loss: 7.7286 - val_acc: 0.0488\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 4.6987 - acc: 0.1540 - val_loss: 7.3091 - val_acc: 0.0430\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 4.2331 - acc: 0.2305 - val_loss: 7.4125 - val_acc: 0.0371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a55928358>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM = generate_LSTM()\n",
    "\n",
    "model_LSTM.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),              validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./checkpoints/GRU_text-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(words),\n",
    "    SEQUENCE_LEN,\n",
    "    MIN_WORD_FREQUENCY\n",
    ")\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "239/239 [==============================] - 10s 43ms/step - loss: 6.3485 - acc: 0.0638 - val_loss: 6.2449 - val_acc: 0.0775\n",
      "Epoch 2/100\n",
      "239/239 [==============================] - 5s 23ms/step - loss: 5.5462 - acc: 0.1106 - val_loss: 6.3037 - val_acc: 0.0984\n",
      "Epoch 3/100\n",
      "239/239 [==============================] - 5s 22ms/step - loss: 4.9776 - acc: 0.1644 - val_loss: 6.6202 - val_acc: 0.1215\n",
      "Epoch 4/100\n",
      "239/239 [==============================] - 5s 22ms/step - loss: 4.5584 - acc: 0.2249 - val_loss: 6.9641 - val_acc: 0.1042\n",
      "Epoch 5/100\n",
      "239/239 [==============================] - 6s 24ms/step - loss: 4.2998 - acc: 0.2737 - val_loss: 7.1916 - val_acc: 0.0995\n",
      "Epoch 6/100\n",
      "239/239 [==============================] - 5s 23ms/step - loss: 4.1761 - acc: 0.3087 - val_loss: 7.2767 - val_acc: 0.0949\n",
      "Epoch 7/100\n",
      "239/239 [==============================] - 6s 24ms/step - loss: 4.0772 - acc: 0.3241 - val_loss: 7.3213 - val_acc: 0.0949\n",
      "Epoch 8/100\n",
      "239/239 [==============================] - 6s 24ms/step - loss: 4.0050 - acc: 0.3453 - val_loss: 7.4230 - val_acc: 0.0891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3c62ce80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM = generate_GRU()\n",
    "\n",
    "model_LSTM.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),              validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
