{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код из лекции\n",
    "### Первая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10000)\n",
      "(5000, 10000)\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:5000]\n",
    "partial_x_train = x_train[5000:]\n",
    "y_val = y_train[:5000]\n",
    "partial_y_train = y_train[5000:]\n",
    "print(partial_x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 4s 178us/step - loss: 0.4715 - acc: 0.8056 - val_loss: 0.3695 - val_acc: 0.8568\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.2722 - acc: 0.9049 - val_loss: 0.2942 - val_acc: 0.8862\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.2027 - acc: 0.9307 - val_loss: 0.2700 - val_acc: 0.8910\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 0.1671 - acc: 0.9415 - val_loss: 0.2799 - val_acc: 0.8900\n",
      "25000/25000 [==============================] - 4s 153us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30405114110946657, 0.87916]\n",
      "0.8809999998807907\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "### с тремя скрытыми слоями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.5295 - acc: 0.7844 - val_loss: 0.3934 - val_acc: 0.8542\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 0.2923 - acc: 0.9042 - val_loss: 0.2911 - val_acc: 0.8910\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.2108 - acc: 0.9274 - val_loss: 0.2938 - val_acc: 0.8830\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.1669 - acc: 0.9419 - val_loss: 0.2844 - val_acc: 0.8886\n",
      "25000/25000 [==============================] - 4s 165us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31313087109565735, 0.87616]\n",
      "0.879200000166893\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# с одним скрытым слоем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.4802 - acc: 0.8140 - val_loss: 0.3691 - val_acc: 0.8764\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.3024 - acc: 0.9043 - val_loss: 0.3024 - val_acc: 0.8856\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.2339 - acc: 0.9246 - val_loss: 0.2799 - val_acc: 0.8916\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.1963 - acc: 0.9355 - val_loss: 0.2923 - val_acc: 0.8830\n",
      "25000/25000 [==============================] - 4s 140us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3138272501039505, 0.87164]\n",
      "0.8841499995231629\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты следуюшие:\n",
    "\n",
    "при одном скрытом слое: test_acc = 0.87164, val_acc = 0.8841499995231629\n",
    "\n",
    "при двух скрытых слоях: test_acc = 0.87916, val_acc = 0.8809999998807907\n",
    "\n",
    "при трех скрытый слоях: test_acc = 0.87616, val_acc = 0.879200000166893\n",
    "\n",
    "Лучший показатель test accuracy - при 2 слоях, validation accuracy - при 1 слое. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "-Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.4525 - acc: 0.8065 - val_loss: 0.3561 - val_acc: 0.8542\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.2527 - acc: 0.9090 - val_loss: 0.3636 - val_acc: 0.8450\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.1926 - acc: 0.9293 - val_loss: 0.2845 - val_acc: 0.8880\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.1528 - acc: 0.9456 - val_loss: 0.2983 - val_acc: 0.8864\n",
      "25000/25000 [==============================] - 4s 158us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3310349017906189, 0.87428]\n",
      "0.8684000006198884\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 4s 178us/step - loss: 0.4560 - acc: 0.7893 - val_loss: 0.3105 - val_acc: 0.8842\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.2435 - acc: 0.9085 - val_loss: 0.3807 - val_acc: 0.8426\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.1803 - acc: 0.9331 - val_loss: 0.3366 - val_acc: 0.8730\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 157us/step - loss: 0.1488 - acc: 0.9446 - val_loss: 0.2831 - val_acc: 0.8920\n",
      "25000/25000 [==============================] - 5s 197us/step\n",
      "[0.3138675223207474, 0.87924]\n",
      "0.8729500000238418\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 4s 208us/step - loss: 0.4473 - acc: 0.7998 - val_loss: 0.3019 - val_acc: 0.8860\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 4s 195us/step - loss: 0.2463 - acc: 0.9086 - val_loss: 0.3054 - val_acc: 0.8756\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1847 - acc: 0.9318 - val_loss: 0.3056 - val_acc: 0.8794\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.1438 - acc: 0.9467 - val_loss: 0.2942 - val_acc: 0.8882\n",
      "25000/25000 [==============================] - 5s 197us/step\n",
      "[0.3236232180309296, 0.87628]\n",
      "0.8822999999046326\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.5089 - acc: 0.7840 - val_loss: 0.3781 - val_acc: 0.8660\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 0.3001 - acc: 0.8990 - val_loss: 0.3250 - val_acc: 0.8680\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.2270 - acc: 0.9259 - val_loss: 0.2770 - val_acc: 0.8932\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.1866 - acc: 0.9383 - val_loss: 0.2764 - val_acc: 0.8890\n",
      "25000/25000 [==============================] - 3s 128us/step\n",
      "[0.29319496866226197, 0.88296]\n",
      "0.8790500003099442\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 156us/step - loss: 0.4688 - acc: 0.8054 - val_loss: 0.3648 - val_acc: 0.8624\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.2775 - acc: 0.9087 - val_loss: 0.2952 - val_acc: 0.8844\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 0.2089 - acc: 0.9304 - val_loss: 0.2688 - val_acc: 0.8930\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.1724 - acc: 0.9428 - val_loss: 0.4168 - val_acc: 0.8396\n",
      "25000/25000 [==============================] - 3s 139us/step\n",
      "[0.4351994038391113, 0.82552]\n",
      "0.8698499992370605\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4879 - acc: 0.8084 - val_loss: 0.3766 - val_acc: 0.8718\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.3063 - acc: 0.9033 - val_loss: 0.3082 - val_acc: 0.8830\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.2403 - acc: 0.9244 - val_loss: 0.2870 - val_acc: 0.8878\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 121us/step - loss: 0.2021 - acc: 0.9350 - val_loss: 0.2734 - val_acc: 0.8918\n",
      "25000/25000 [==============================] - 3s 127us/step\n",
      "[0.29175043167114256, 0.88292]\n",
      "0.8835999998092651\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод по hidden units для данных данных:\n",
    "- чем их меньше, тем лучше для test accuracy\n",
    "- при двух слоях модель наилучшие показатели test accuracy = 0.88296 при 8 нейронах\n",
    "- но test accuracy еще лучше, если в модели один скрытый слой с 8 нейронами (0,88292)\n",
    "- validation accuracy наилучший при 1 слое и 8 нейронах (0.8835999998092651)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "-Try using the mse loss function instead of binary_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 155us/step - loss: 0.1504 - acc: 0.8098 - val_loss: 0.1108 - val_acc: 0.8598\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 122us/step - loss: 0.0792 - acc: 0.9098 - val_loss: 0.0911 - val_acc: 0.8790\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 0.0586 - acc: 0.9339 - val_loss: 0.0860 - val_acc: 0.8840\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.0470 - acc: 0.9469 - val_loss: 0.0836 - val_acc: 0.8886\n",
      "25000/25000 [==============================] - 4s 144us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09008973398208618, 0.87684]\n",
      "0.877850000333786\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: \n",
    "- при binary_crossentropy\n",
    "- test accuracy = 0.87916, val_acc = 0.8809999998807907\n",
    "- при mse\n",
    "- test accuracy = 0.87684, val_acc = 0.877850000333786\n",
    "- лучше при binary_crossentropy, но разница в тысячных долях\n",
    "______________________\n",
    "-Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 3s 159us/step - loss: 0.4698 - acc: 0.8165 - val_loss: 0.3422 - val_acc: 0.8796\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.2656 - acc: 0.9106 - val_loss: 0.2959 - val_acc: 0.8818\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.1922 - acc: 0.9324 - val_loss: 0.2859 - val_acc: 0.8838\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 124us/step - loss: 0.1503 - acc: 0.9479 - val_loss: 0.2991 - val_acc: 0.8844\n",
      "25000/25000 [==============================] - 3s 139us/step\n",
      "[0.3243111707830429, 0.87588]\n",
      "0.8823999998092651\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relu: test accuracy = 0.87916, val_acc = 0.8809999998807907\n",
    "- tanh: test accuracy = 0.87588, val_acc = 0.8823999998092651\n",
    "при relu test accuracy лучше, а val_acc хуже, но разница опять в тысячных долях "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая часть "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код из лекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 3s 328us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 2s 190us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 154us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 144us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 150us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 146us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "2246/2246 [==============================] - 0s 216us/step\n",
      "[0.9810770948551005, 0.7880676759212865]\n",
      "0.756111107243432\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try using larger or smaller layers: 32 units, 128 units, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s 225us/step - loss: 3.0975 - acc: 0.4546 - val_loss: 2.3165 - val_acc: 0.6090\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 1.8966 - acc: 0.6625 - val_loss: 1.6682 - val_acc: 0.6590\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 1.4035 - acc: 0.7175 - val_loss: 1.4031 - val_acc: 0.6950\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 1.1576 - acc: 0.7502 - val_loss: 1.2633 - val_acc: 0.7280\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 129us/step - loss: 0.9929 - acc: 0.7890 - val_loss: 1.1801 - val_acc: 0.7520\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 124us/step - loss: 0.8635 - acc: 0.8166 - val_loss: 1.1190 - val_acc: 0.7720\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 0.7562 - acc: 0.8356 - val_loss: 1.0802 - val_acc: 0.7700\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.6663 - acc: 0.8539 - val_loss: 1.0447 - val_acc: 0.7820\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 129us/step - loss: 0.5829 - acc: 0.8733 - val_loss: 1.0104 - val_acc: 0.7980\n",
      "2246/2246 [==============================] - 0s 163us/step\n",
      "[1.0722839076499906, 0.7644701692427468]\n",
      "0.7294444450272455\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 3s 332us/step - loss: 2.2531 - acc: 0.5534 - val_loss: 1.4524 - val_acc: 0.6630\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s 237us/step - loss: 1.1717 - acc: 0.7428 - val_loss: 1.1415 - val_acc: 0.7490\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 0.8202 - acc: 0.8260 - val_loss: 1.0116 - val_acc: 0.7760\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 2s 238us/step - loss: 0.5955 - acc: 0.8776 - val_loss: 0.9961 - val_acc: 0.7720\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 2s 232us/step - loss: 0.4333 - acc: 0.9114 - val_loss: 0.9036 - val_acc: 0.7960\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 2s 242us/step - loss: 0.3378 - acc: 0.9263 - val_loss: 0.8720 - val_acc: 0.8190\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 0.2604 - acc: 0.9411 - val_loss: 0.8659 - val_acc: 0.8270\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 2s 231us/step - loss: 0.2148 - acc: 0.9471 - val_loss: 0.9802 - val_acc: 0.8020\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 2s 231us/step - loss: 0.1816 - acc: 0.9524 - val_loss: 1.0121 - val_acc: 0.7910\n",
      "2246/2246 [==============================] - 1s 319us/step\n",
      "[1.1203325525098684, 0.7684772929652716]\n",
      "0.7772222203678555\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 4s 439us/step - loss: 2.0758 - acc: 0.5719 - val_loss: 1.2636 - val_acc: 0.7210\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s 276us/step - loss: 0.9763 - acc: 0.7873 - val_loss: 1.0148 - val_acc: 0.7710\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 2s 311us/step - loss: 0.6277 - acc: 0.8653 - val_loss: 0.8761 - val_acc: 0.8160\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 3s 341us/step - loss: 0.4272 - acc: 0.9097 - val_loss: 0.8602 - val_acc: 0.8100\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 3s 342us/step - loss: 0.3064 - acc: 0.9332 - val_loss: 0.8797 - val_acc: 0.8190\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 3s 347us/step - loss: 0.2276 - acc: 0.9476 - val_loss: 0.8883 - val_acc: 0.8220\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 3s 348us/step - loss: 0.1995 - acc: 0.9470 - val_loss: 0.8960 - val_acc: 0.8240\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 3s 342us/step - loss: 0.1683 - acc: 0.9515 - val_loss: 0.9315 - val_acc: 0.8070\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 2s 308us/step - loss: 0.1513 - acc: 0.9539 - val_loss: 0.8932 - val_acc: 0.8270\n",
      "2246/2246 [==============================] - 1s 243us/step\n",
      "[1.0265171071406676, 0.800979519145147]\n",
      "0.8018888918558756\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 3s 385us/step - loss: 2.2850 - acc: 0.5593 - val_loss: 1.4532 - val_acc: 0.6740\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s 232us/step - loss: 1.1340 - acc: 0.7521 - val_loss: 1.1224 - val_acc: 0.7580\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 2s 209us/step - loss: 0.7916 - acc: 0.8314 - val_loss: 0.9898 - val_acc: 0.7870\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 2s 221us/step - loss: 0.5786 - acc: 0.8822 - val_loss: 0.9744 - val_acc: 0.7890\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 2s 233us/step - loss: 0.4247 - acc: 0.9118 - val_loss: 0.9210 - val_acc: 0.7990\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 2s 242us/step - loss: 0.3330 - acc: 0.9316 - val_loss: 0.8707 - val_acc: 0.8210\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.2629 - acc: 0.9401 - val_loss: 0.8734 - val_acc: 0.8260\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 2s 218us/step - loss: 0.2228 - acc: 0.9471 - val_loss: 0.8930 - val_acc: 0.8220\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 0.1866 - acc: 0.9521 - val_loss: 0.8836 - val_acc: 0.8170\n",
      "2246/2246 [==============================] - 1s 267us/step\n",
      "[1.0345304177896528, 0.7889581478448818]\n",
      "0.7881111114819844\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 32 нейрона: test_acc = 0.7622439893143366, val_acc = 0.7176666611565484\n",
    "- 64 нейрона: test_acc = 0.7880676759212865, val_acc = 0.756111107243432\n",
    "- 128 нейронов: test_acc = 0.7684772929652716, val_acc = 0.7772222203678555\n",
    "- 128(1) - 64(2) нейрона: test_acc = 0.7898486197950154, val_acc = 0.7880000003708734\n",
    "- 256(1) - 128(2) нейронов: test_acc = 0.800979519145147, val_acc = 0.8018888918558756 - лучшие показатели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You used two hidden layers. Now try using a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 2.6321 - acc: 0.5487 - val_loss: 1.8462 - val_acc: 0.6360\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 1.5211 - acc: 0.7127 - val_loss: 1.3738 - val_acc: 0.7100\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 1.1090 - acc: 0.7772 - val_loss: 1.1511 - val_acc: 0.7450\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.8635 - acc: 0.8256 - val_loss: 1.0251 - val_acc: 0.7860\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.6935 - acc: 0.8626 - val_loss: 0.9478 - val_acc: 0.8010\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.5650 - acc: 0.8918 - val_loss: 0.8866 - val_acc: 0.8230\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.4650 - acc: 0.9100 - val_loss: 0.8535 - val_acc: 0.8160\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.3875 - acc: 0.9227 - val_loss: 0.8352 - val_acc: 0.8210\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.3260 - acc: 0.9336 - val_loss: 0.8192 - val_acc: 0.8260\n",
      "2246/2246 [==============================] - 0s 203us/step\n",
      "[0.8927820435827158, 0.8023152271234235]\n",
      "0.7737777768241035\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s 264us/step - loss: 2.6430 - acc: 0.4728 - val_loss: 1.7158 - val_acc: 0.6230\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 147us/step - loss: 1.4251 - acc: 0.6850 - val_loss: 1.3346 - val_acc: 0.7160\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 1.0801 - acc: 0.7548 - val_loss: 1.1593 - val_acc: 0.7430\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 0.8684 - acc: 0.8043 - val_loss: 1.0689 - val_acc: 0.7640\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 144us/step - loss: 0.6944 - acc: 0.8444 - val_loss: 1.0847 - val_acc: 0.7670\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 144us/step - loss: 0.5572 - acc: 0.8753 - val_loss: 1.0184 - val_acc: 0.7890\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 0.4561 - acc: 0.8988 - val_loss: 0.9697 - val_acc: 0.8030\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 0.3608 - acc: 0.9225 - val_loss: 0.9895 - val_acc: 0.8070\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 0.3005 - acc: 0.9356 - val_loss: 1.0152 - val_acc: 0.7980\n",
      "2246/2246 [==============================] - 1s 236us/step\n",
      "[1.0872287493355022, 0.7756010685663401]\n",
      "0.7566666629579332\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)\n",
    "print(np.array(history.history['val_acc']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 1 слой: test_acc = 0.8023152271234235, val_acc = 0.7737777768241035 - лучшие показатели\n",
    " - 2 слоя: test_acc = 0.7880676759212865, val_acc = 0.756111107243432\n",
    " - 3 слой: test_acc = 0.7756010685663401, val_acc = 0.7566666629579332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общий вывод: лучшие показатели - это, конечно, хорошо, но переобучуться не хочется, тем более нагружать модель ради сотых или тысячных доль значений на несложных данных нет смысла. Поэтому в первой части ноутбука лучше остановиться на 2 слоях и 16 нейронках, а во второй части - на 2 слоях и 64 нейронках (при тех же показателях, которые были на лекции). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
