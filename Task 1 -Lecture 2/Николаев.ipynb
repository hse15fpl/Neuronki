{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 - Николаев\n",
    "Попытка выполнить в PyTorch, ибо нефиг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor # для переноса нагрузки на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (explanation from a website): Data loading in PyTorch is in 2 parts:\n",
    "\n",
    "First the data must be wrapped in a Dataset class with a getitem method that from an index return X_train[index] and y_train[index] and a length method. A Dataset is basically a data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDatasetTrain(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X_train = vectorize_sequences(data)\n",
    "        self.y_train = np.asarray(labels).astype('float32')\n",
    "    def __getitem__(self, index):\n",
    "        item = self.X_train[index]\n",
    "        label = self.y_train[index]\n",
    "        return item, label\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = IMDBDatasetTrain(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part: we provided PyTorch with a data storage, and we have to tell it how to load it. This is done with DataLoader.\n",
    "\n",
    "The DataLoader defines how you retrieve the items + labels from the dataset. You can tell it to:\n",
    "\n",
    "Set the batch size.\n",
    "Shuffle and sample the data randomly, hence implementing train_test_split (as in SubsetRandomSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dset_train,\n",
    "                          batch_size = 512,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 0, # 4 for CUDA, 1 for Ubuntu, 0 for Windows\n",
    "                          pin_memory = True)  # unspecified for CPU, specified for CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная сетка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "model = Net().cuda() # for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).type(dtype), Variable(target).type(dtype)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1594: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.704275\n",
      "Train Epoch: 1 [1024/25000 (4%)]\tLoss: 0.693540\n",
      "Train Epoch: 1 [2048/25000 (8%)]\tLoss: 0.650261\n",
      "Train Epoch: 1 [3072/25000 (12%)]\tLoss: 0.621235\n",
      "Train Epoch: 1 [4096/25000 (16%)]\tLoss: 0.593387\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 0.575729\n",
      "Train Epoch: 1 [6144/25000 (24%)]\tLoss: 0.564508\n",
      "Train Epoch: 1 [7168/25000 (29%)]\tLoss: 0.563516\n",
      "Train Epoch: 1 [8192/25000 (33%)]\tLoss: 0.547607\n",
      "Train Epoch: 1 [9216/25000 (37%)]\tLoss: 0.547469\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 0.487480\n",
      "Train Epoch: 1 [11264/25000 (45%)]\tLoss: 0.525624\n",
      "Train Epoch: 1 [12288/25000 (49%)]\tLoss: 0.484419\n",
      "Train Epoch: 1 [13312/25000 (53%)]\tLoss: 0.496202\n",
      "Train Epoch: 1 [14336/25000 (57%)]\tLoss: 0.487536\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 0.476722\n",
      "Train Epoch: 1 [16384/25000 (65%)]\tLoss: 0.494039\n",
      "Train Epoch: 1 [17408/25000 (69%)]\tLoss: 0.454751\n",
      "Train Epoch: 1 [18432/25000 (73%)]\tLoss: 0.458144\n",
      "Train Epoch: 1 [19456/25000 (78%)]\tLoss: 0.450416\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 0.437953\n",
      "Train Epoch: 1 [21504/25000 (86%)]\tLoss: 0.425940\n",
      "Train Epoch: 1 [22528/25000 (90%)]\tLoss: 0.443841\n",
      "Train Epoch: 1 [23552/25000 (94%)]\tLoss: 0.420462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1594: UserWarning: Using a target size (torch.Size([424])) that is different to the input size (torch.Size([424, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [20352/25000 (98%)]\tLoss: 0.442669\n",
      "Train Epoch: 2 [0/25000 (0%)]\tLoss: 0.440060\n",
      "Train Epoch: 2 [1024/25000 (4%)]\tLoss: 0.411390\n",
      "Train Epoch: 2 [2048/25000 (8%)]\tLoss: 0.396730\n",
      "Train Epoch: 2 [3072/25000 (12%)]\tLoss: 0.406964\n",
      "Train Epoch: 2 [4096/25000 (16%)]\tLoss: 0.420761\n",
      "Train Epoch: 2 [5120/25000 (20%)]\tLoss: 0.386928\n",
      "Train Epoch: 2 [6144/25000 (24%)]\tLoss: 0.378594\n",
      "Train Epoch: 2 [7168/25000 (29%)]\tLoss: 0.381667\n",
      "Train Epoch: 2 [8192/25000 (33%)]\tLoss: 0.387508\n",
      "Train Epoch: 2 [9216/25000 (37%)]\tLoss: 0.381449\n",
      "Train Epoch: 2 [10240/25000 (41%)]\tLoss: 0.430474\n",
      "Train Epoch: 2 [11264/25000 (45%)]\tLoss: 0.394094\n",
      "Train Epoch: 2 [12288/25000 (49%)]\tLoss: 0.395607\n",
      "Train Epoch: 2 [13312/25000 (53%)]\tLoss: 0.397236\n",
      "Train Epoch: 2 [14336/25000 (57%)]\tLoss: 0.372592\n",
      "Train Epoch: 2 [15360/25000 (61%)]\tLoss: 0.381235\n",
      "Train Epoch: 2 [16384/25000 (65%)]\tLoss: 0.388856\n",
      "Train Epoch: 2 [17408/25000 (69%)]\tLoss: 0.362407\n",
      "Train Epoch: 2 [18432/25000 (73%)]\tLoss: 0.373532\n",
      "Train Epoch: 2 [19456/25000 (78%)]\tLoss: 0.358101\n",
      "Train Epoch: 2 [20480/25000 (82%)]\tLoss: 0.372671\n",
      "Train Epoch: 2 [21504/25000 (86%)]\tLoss: 0.417677\n",
      "Train Epoch: 2 [22528/25000 (90%)]\tLoss: 0.377749\n",
      "Train Epoch: 2 [23552/25000 (94%)]\tLoss: 0.353684\n",
      "Train Epoch: 2 [20352/25000 (98%)]\tLoss: 0.344003\n",
      "Train Epoch: 3 [0/25000 (0%)]\tLoss: 0.332116\n",
      "Train Epoch: 3 [1024/25000 (4%)]\tLoss: 0.351022\n",
      "Train Epoch: 3 [2048/25000 (8%)]\tLoss: 0.326665\n",
      "Train Epoch: 3 [3072/25000 (12%)]\tLoss: 0.329826\n",
      "Train Epoch: 3 [4096/25000 (16%)]\tLoss: 0.330547\n",
      "Train Epoch: 3 [5120/25000 (20%)]\tLoss: 0.314753\n",
      "Train Epoch: 3 [6144/25000 (24%)]\tLoss: 0.310294\n",
      "Train Epoch: 3 [7168/25000 (29%)]\tLoss: 0.311772\n",
      "Train Epoch: 3 [8192/25000 (33%)]\tLoss: 0.314638\n",
      "Train Epoch: 3 [9216/25000 (37%)]\tLoss: 0.309702\n",
      "Train Epoch: 3 [10240/25000 (41%)]\tLoss: 0.298564\n",
      "Train Epoch: 3 [11264/25000 (45%)]\tLoss: 0.302891\n",
      "Train Epoch: 3 [12288/25000 (49%)]\tLoss: 0.341176\n",
      "Train Epoch: 3 [13312/25000 (53%)]\tLoss: 0.311046\n",
      "Train Epoch: 3 [14336/25000 (57%)]\tLoss: 0.351317\n",
      "Train Epoch: 3 [15360/25000 (61%)]\tLoss: 0.306668\n",
      "Train Epoch: 3 [16384/25000 (65%)]\tLoss: 0.301286\n",
      "Train Epoch: 3 [17408/25000 (69%)]\tLoss: 0.330258\n",
      "Train Epoch: 3 [18432/25000 (73%)]\tLoss: 0.289299\n",
      "Train Epoch: 3 [19456/25000 (78%)]\tLoss: 0.331965\n",
      "Train Epoch: 3 [20480/25000 (82%)]\tLoss: 0.357894\n",
      "Train Epoch: 3 [21504/25000 (86%)]\tLoss: 0.309732\n",
      "Train Epoch: 3 [22528/25000 (90%)]\tLoss: 0.322052\n",
      "Train Epoch: 3 [23552/25000 (94%)]\tLoss: 0.302671\n",
      "Train Epoch: 3 [20352/25000 (98%)]\tLoss: 0.315463\n",
      "Train Epoch: 4 [0/25000 (0%)]\tLoss: 0.274607\n",
      "Train Epoch: 4 [1024/25000 (4%)]\tLoss: 0.309532\n",
      "Train Epoch: 4 [2048/25000 (8%)]\tLoss: 0.272635\n",
      "Train Epoch: 4 [3072/25000 (12%)]\tLoss: 0.278527\n",
      "Train Epoch: 4 [4096/25000 (16%)]\tLoss: 0.330765\n",
      "Train Epoch: 4 [5120/25000 (20%)]\tLoss: 0.263835\n",
      "Train Epoch: 4 [6144/25000 (24%)]\tLoss: 0.301416\n",
      "Train Epoch: 4 [7168/25000 (29%)]\tLoss: 0.293161\n",
      "Train Epoch: 4 [8192/25000 (33%)]\tLoss: 0.286925\n",
      "Train Epoch: 4 [9216/25000 (37%)]\tLoss: 0.271769\n",
      "Train Epoch: 4 [10240/25000 (41%)]\tLoss: 0.291536\n",
      "Train Epoch: 4 [11264/25000 (45%)]\tLoss: 0.273071\n",
      "Train Epoch: 4 [12288/25000 (49%)]\tLoss: 0.290468\n",
      "Train Epoch: 4 [13312/25000 (53%)]\tLoss: 0.276937\n",
      "Train Epoch: 4 [14336/25000 (57%)]\tLoss: 0.289242\n",
      "Train Epoch: 4 [15360/25000 (61%)]\tLoss: 0.295293\n",
      "Train Epoch: 4 [16384/25000 (65%)]\tLoss: 0.259406\n",
      "Train Epoch: 4 [17408/25000 (69%)]\tLoss: 0.282343\n",
      "Train Epoch: 4 [18432/25000 (73%)]\tLoss: 0.282526\n",
      "Train Epoch: 4 [19456/25000 (78%)]\tLoss: 0.265729\n",
      "Train Epoch: 4 [20480/25000 (82%)]\tLoss: 0.261665\n",
      "Train Epoch: 4 [21504/25000 (86%)]\tLoss: 0.271579\n",
      "Train Epoch: 4 [22528/25000 (90%)]\tLoss: 0.276894\n",
      "Train Epoch: 4 [23552/25000 (94%)]\tLoss: 0.257168\n",
      "Train Epoch: 4 [20352/25000 (98%)]\tLoss: 0.242156\n",
      "Train Epoch: 5 [0/25000 (0%)]\tLoss: 0.278366\n",
      "Train Epoch: 5 [1024/25000 (4%)]\tLoss: 0.252647\n",
      "Train Epoch: 5 [2048/25000 (8%)]\tLoss: 0.240071\n",
      "Train Epoch: 5 [3072/25000 (12%)]\tLoss: 0.274990\n",
      "Train Epoch: 5 [4096/25000 (16%)]\tLoss: 0.227709\n",
      "Train Epoch: 5 [5120/25000 (20%)]\tLoss: 0.241425\n",
      "Train Epoch: 5 [6144/25000 (24%)]\tLoss: 0.257005\n",
      "Train Epoch: 5 [7168/25000 (29%)]\tLoss: 0.245490\n",
      "Train Epoch: 5 [8192/25000 (33%)]\tLoss: 0.225895\n",
      "Train Epoch: 5 [9216/25000 (37%)]\tLoss: 0.260805\n",
      "Train Epoch: 5 [10240/25000 (41%)]\tLoss: 0.224360\n",
      "Train Epoch: 5 [11264/25000 (45%)]\tLoss: 0.230728\n",
      "Train Epoch: 5 [12288/25000 (49%)]\tLoss: 0.229115\n",
      "Train Epoch: 5 [13312/25000 (53%)]\tLoss: 0.221440\n",
      "Train Epoch: 5 [14336/25000 (57%)]\tLoss: 0.226282\n",
      "Train Epoch: 5 [15360/25000 (61%)]\tLoss: 0.224645\n",
      "Train Epoch: 5 [16384/25000 (65%)]\tLoss: 0.224632\n",
      "Train Epoch: 5 [17408/25000 (69%)]\tLoss: 0.238808\n",
      "Train Epoch: 5 [18432/25000 (73%)]\tLoss: 0.259378\n",
      "Train Epoch: 5 [19456/25000 (78%)]\tLoss: 0.269116\n",
      "Train Epoch: 5 [20480/25000 (82%)]\tLoss: 0.271873\n",
      "Train Epoch: 5 [21504/25000 (86%)]\tLoss: 0.246293\n",
      "Train Epoch: 5 [22528/25000 (90%)]\tLoss: 0.251796\n",
      "Train Epoch: 5 [23552/25000 (94%)]\tLoss: 0.237096\n",
      "Train Epoch: 5 [20352/25000 (98%)]\tLoss: 0.210976\n",
      "Train Epoch: 6 [0/25000 (0%)]\tLoss: 0.239182\n",
      "Train Epoch: 6 [1024/25000 (4%)]\tLoss: 0.223820\n",
      "Train Epoch: 6 [2048/25000 (8%)]\tLoss: 0.227663\n",
      "Train Epoch: 6 [3072/25000 (12%)]\tLoss: 0.217206\n",
      "Train Epoch: 6 [4096/25000 (16%)]\tLoss: 0.240781\n",
      "Train Epoch: 6 [5120/25000 (20%)]\tLoss: 0.188232\n",
      "Train Epoch: 6 [6144/25000 (24%)]\tLoss: 0.231336\n",
      "Train Epoch: 6 [7168/25000 (29%)]\tLoss: 0.217860\n",
      "Train Epoch: 6 [8192/25000 (33%)]\tLoss: 0.211320\n",
      "Train Epoch: 6 [9216/25000 (37%)]\tLoss: 0.191490\n",
      "Train Epoch: 6 [10240/25000 (41%)]\tLoss: 0.199551\n",
      "Train Epoch: 6 [11264/25000 (45%)]\tLoss: 0.203388\n",
      "Train Epoch: 6 [12288/25000 (49%)]\tLoss: 0.235189\n",
      "Train Epoch: 6 [13312/25000 (53%)]\tLoss: 0.224988\n",
      "Train Epoch: 6 [14336/25000 (57%)]\tLoss: 0.196677\n",
      "Train Epoch: 6 [15360/25000 (61%)]\tLoss: 0.209925\n",
      "Train Epoch: 6 [16384/25000 (65%)]\tLoss: 0.207618\n",
      "Train Epoch: 6 [17408/25000 (69%)]\tLoss: 0.225468\n",
      "Train Epoch: 6 [18432/25000 (73%)]\tLoss: 0.245759\n",
      "Train Epoch: 6 [19456/25000 (78%)]\tLoss: 0.243549\n",
      "Train Epoch: 6 [20480/25000 (82%)]\tLoss: 0.210138\n",
      "Train Epoch: 6 [21504/25000 (86%)]\tLoss: 0.214307\n",
      "Train Epoch: 6 [22528/25000 (90%)]\tLoss: 0.207921\n",
      "Train Epoch: 6 [23552/25000 (94%)]\tLoss: 0.282576\n",
      "Train Epoch: 6 [20352/25000 (98%)]\tLoss: 0.226738\n",
      "Train Epoch: 7 [0/25000 (0%)]\tLoss: 0.202171\n",
      "Train Epoch: 7 [1024/25000 (4%)]\tLoss: 0.210255\n",
      "Train Epoch: 7 [2048/25000 (8%)]\tLoss: 0.171323\n",
      "Train Epoch: 7 [3072/25000 (12%)]\tLoss: 0.200818\n",
      "Train Epoch: 7 [4096/25000 (16%)]\tLoss: 0.177006\n",
      "Train Epoch: 7 [5120/25000 (20%)]\tLoss: 0.232529\n",
      "Train Epoch: 7 [6144/25000 (24%)]\tLoss: 0.232534\n",
      "Train Epoch: 7 [7168/25000 (29%)]\tLoss: 0.166072\n",
      "Train Epoch: 7 [8192/25000 (33%)]\tLoss: 0.217984\n",
      "Train Epoch: 7 [9216/25000 (37%)]\tLoss: 0.225032\n",
      "Train Epoch: 7 [10240/25000 (41%)]\tLoss: 0.198010\n",
      "Train Epoch: 7 [11264/25000 (45%)]\tLoss: 0.218840\n",
      "Train Epoch: 7 [12288/25000 (49%)]\tLoss: 0.213120\n",
      "Train Epoch: 7 [13312/25000 (53%)]\tLoss: 0.209272\n",
      "Train Epoch: 7 [14336/25000 (57%)]\tLoss: 0.213796\n",
      "Train Epoch: 7 [15360/25000 (61%)]\tLoss: 0.204206\n",
      "Train Epoch: 7 [16384/25000 (65%)]\tLoss: 0.168763\n",
      "Train Epoch: 7 [17408/25000 (69%)]\tLoss: 0.219635\n",
      "Train Epoch: 7 [18432/25000 (73%)]\tLoss: 0.207059\n",
      "Train Epoch: 7 [19456/25000 (78%)]\tLoss: 0.215844\n",
      "Train Epoch: 7 [20480/25000 (82%)]\tLoss: 0.189583\n",
      "Train Epoch: 7 [21504/25000 (86%)]\tLoss: 0.208248\n",
      "Train Epoch: 7 [22528/25000 (90%)]\tLoss: 0.211707\n",
      "Train Epoch: 7 [23552/25000 (94%)]\tLoss: 0.195353\n",
      "Train Epoch: 7 [20352/25000 (98%)]\tLoss: 0.212211\n",
      "Train Epoch: 8 [0/25000 (0%)]\tLoss: 0.202055\n",
      "Train Epoch: 8 [1024/25000 (4%)]\tLoss: 0.184860\n",
      "Train Epoch: 8 [2048/25000 (8%)]\tLoss: 0.180354\n",
      "Train Epoch: 8 [3072/25000 (12%)]\tLoss: 0.218444\n",
      "Train Epoch: 8 [4096/25000 (16%)]\tLoss: 0.172068\n",
      "Train Epoch: 8 [5120/25000 (20%)]\tLoss: 0.206570\n",
      "Train Epoch: 8 [6144/25000 (24%)]\tLoss: 0.185388\n",
      "Train Epoch: 8 [7168/25000 (29%)]\tLoss: 0.181678\n",
      "Train Epoch: 8 [8192/25000 (33%)]\tLoss: 0.191133\n",
      "Train Epoch: 8 [9216/25000 (37%)]\tLoss: 0.199045\n",
      "Train Epoch: 8 [10240/25000 (41%)]\tLoss: 0.220629\n",
      "Train Epoch: 8 [11264/25000 (45%)]\tLoss: 0.197477\n",
      "Train Epoch: 8 [12288/25000 (49%)]\tLoss: 0.206336\n",
      "Train Epoch: 8 [13312/25000 (53%)]\tLoss: 0.210924\n",
      "Train Epoch: 8 [14336/25000 (57%)]\tLoss: 0.203859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [15360/25000 (61%)]\tLoss: 0.184613\n",
      "Train Epoch: 8 [16384/25000 (65%)]\tLoss: 0.207427\n",
      "Train Epoch: 8 [17408/25000 (69%)]\tLoss: 0.219065\n",
      "Train Epoch: 8 [18432/25000 (73%)]\tLoss: 0.190829\n",
      "Train Epoch: 8 [19456/25000 (78%)]\tLoss: 0.169501\n",
      "Train Epoch: 8 [20480/25000 (82%)]\tLoss: 0.179655\n",
      "Train Epoch: 8 [21504/25000 (86%)]\tLoss: 0.184610\n",
      "Train Epoch: 8 [22528/25000 (90%)]\tLoss: 0.229119\n",
      "Train Epoch: 8 [23552/25000 (94%)]\tLoss: 0.216136\n",
      "Train Epoch: 8 [20352/25000 (98%)]\tLoss: 0.182454\n",
      "Train Epoch: 9 [0/25000 (0%)]\tLoss: 0.162590\n",
      "Train Epoch: 9 [1024/25000 (4%)]\tLoss: 0.180806\n",
      "Train Epoch: 9 [2048/25000 (8%)]\tLoss: 0.157008\n",
      "Train Epoch: 9 [3072/25000 (12%)]\tLoss: 0.218022\n",
      "Train Epoch: 9 [4096/25000 (16%)]\tLoss: 0.196235\n",
      "Train Epoch: 9 [5120/25000 (20%)]\tLoss: 0.170829\n",
      "Train Epoch: 9 [6144/25000 (24%)]\tLoss: 0.188421\n",
      "Train Epoch: 9 [7168/25000 (29%)]\tLoss: 0.157828\n",
      "Train Epoch: 9 [8192/25000 (33%)]\tLoss: 0.179668\n",
      "Train Epoch: 9 [9216/25000 (37%)]\tLoss: 0.176962\n",
      "Train Epoch: 9 [10240/25000 (41%)]\tLoss: 0.193169\n",
      "Train Epoch: 9 [11264/25000 (45%)]\tLoss: 0.181732\n",
      "Train Epoch: 9 [12288/25000 (49%)]\tLoss: 0.201684\n",
      "Train Epoch: 9 [13312/25000 (53%)]\tLoss: 0.152371\n",
      "Train Epoch: 9 [14336/25000 (57%)]\tLoss: 0.187106\n",
      "Train Epoch: 9 [15360/25000 (61%)]\tLoss: 0.185775\n",
      "Train Epoch: 9 [16384/25000 (65%)]\tLoss: 0.181747\n",
      "Train Epoch: 9 [17408/25000 (69%)]\tLoss: 0.174027\n",
      "Train Epoch: 9 [18432/25000 (73%)]\tLoss: 0.209627\n",
      "Train Epoch: 9 [19456/25000 (78%)]\tLoss: 0.174078\n",
      "Train Epoch: 9 [20480/25000 (82%)]\tLoss: 0.171873\n",
      "Train Epoch: 9 [21504/25000 (86%)]\tLoss: 0.185492\n",
      "Train Epoch: 9 [22528/25000 (90%)]\tLoss: 0.170075\n",
      "Train Epoch: 9 [23552/25000 (94%)]\tLoss: 0.199158\n",
      "Train Epoch: 9 [20352/25000 (98%)]\tLoss: 0.196624\n",
      "Train Epoch: 10 [0/25000 (0%)]\tLoss: 0.153127\n",
      "Train Epoch: 10 [1024/25000 (4%)]\tLoss: 0.178754\n",
      "Train Epoch: 10 [2048/25000 (8%)]\tLoss: 0.183306\n",
      "Train Epoch: 10 [3072/25000 (12%)]\tLoss: 0.167129\n",
      "Train Epoch: 10 [4096/25000 (16%)]\tLoss: 0.175510\n",
      "Train Epoch: 10 [5120/25000 (20%)]\tLoss: 0.163588\n",
      "Train Epoch: 10 [6144/25000 (24%)]\tLoss: 0.176039\n",
      "Train Epoch: 10 [7168/25000 (29%)]\tLoss: 0.145230\n",
      "Train Epoch: 10 [8192/25000 (33%)]\tLoss: 0.176778\n",
      "Train Epoch: 10 [9216/25000 (37%)]\tLoss: 0.164873\n",
      "Train Epoch: 10 [10240/25000 (41%)]\tLoss: 0.162160\n",
      "Train Epoch: 10 [11264/25000 (45%)]\tLoss: 0.144844\n",
      "Train Epoch: 10 [12288/25000 (49%)]\tLoss: 0.166100\n",
      "Train Epoch: 10 [13312/25000 (53%)]\tLoss: 0.172618\n",
      "Train Epoch: 10 [14336/25000 (57%)]\tLoss: 0.175943\n",
      "Train Epoch: 10 [15360/25000 (61%)]\tLoss: 0.150132\n",
      "Train Epoch: 10 [16384/25000 (65%)]\tLoss: 0.158612\n",
      "Train Epoch: 10 [17408/25000 (69%)]\tLoss: 0.175171\n",
      "Train Epoch: 10 [18432/25000 (73%)]\tLoss: 0.167229\n",
      "Train Epoch: 10 [19456/25000 (78%)]\tLoss: 0.157120\n",
      "Train Epoch: 10 [20480/25000 (82%)]\tLoss: 0.149872\n",
      "Train Epoch: 10 [21504/25000 (86%)]\tLoss: 0.155083\n",
      "Train Epoch: 10 [22528/25000 (90%)]\tLoss: 0.184745\n",
      "Train Epoch: 10 [23552/25000 (94%)]\tLoss: 0.166062\n",
      "Train Epoch: 10 [20352/25000 (98%)]\tLoss: 0.184115\n",
      "Train Epoch: 11 [0/25000 (0%)]\tLoss: 0.137102\n",
      "Train Epoch: 11 [1024/25000 (4%)]\tLoss: 0.138716\n",
      "Train Epoch: 11 [2048/25000 (8%)]\tLoss: 0.172659\n",
      "Train Epoch: 11 [3072/25000 (12%)]\tLoss: 0.137756\n",
      "Train Epoch: 11 [4096/25000 (16%)]\tLoss: 0.164257\n",
      "Train Epoch: 11 [5120/25000 (20%)]\tLoss: 0.143042\n",
      "Train Epoch: 11 [6144/25000 (24%)]\tLoss: 0.139237\n",
      "Train Epoch: 11 [7168/25000 (29%)]\tLoss: 0.145058\n",
      "Train Epoch: 11 [8192/25000 (33%)]\tLoss: 0.149700\n",
      "Train Epoch: 11 [9216/25000 (37%)]\tLoss: 0.143039\n",
      "Train Epoch: 11 [10240/25000 (41%)]\tLoss: 0.146615\n",
      "Train Epoch: 11 [11264/25000 (45%)]\tLoss: 0.159398\n",
      "Train Epoch: 11 [12288/25000 (49%)]\tLoss: 0.177083\n",
      "Train Epoch: 11 [13312/25000 (53%)]\tLoss: 0.151271\n",
      "Train Epoch: 11 [14336/25000 (57%)]\tLoss: 0.140754\n",
      "Train Epoch: 11 [15360/25000 (61%)]\tLoss: 0.130778\n",
      "Train Epoch: 11 [16384/25000 (65%)]\tLoss: 0.184314\n",
      "Train Epoch: 11 [17408/25000 (69%)]\tLoss: 0.184424\n",
      "Train Epoch: 11 [18432/25000 (73%)]\tLoss: 0.157374\n",
      "Train Epoch: 11 [19456/25000 (78%)]\tLoss: 0.136573\n",
      "Train Epoch: 11 [20480/25000 (82%)]\tLoss: 0.159168\n",
      "Train Epoch: 11 [21504/25000 (86%)]\tLoss: 0.164211\n",
      "Train Epoch: 11 [22528/25000 (90%)]\tLoss: 0.175372\n",
      "Train Epoch: 11 [23552/25000 (94%)]\tLoss: 0.113289\n",
      "Train Epoch: 11 [20352/25000 (98%)]\tLoss: 0.162600\n",
      "Train Epoch: 12 [0/25000 (0%)]\tLoss: 0.134551\n",
      "Train Epoch: 12 [1024/25000 (4%)]\tLoss: 0.147366\n",
      "Train Epoch: 12 [2048/25000 (8%)]\tLoss: 0.161025\n",
      "Train Epoch: 12 [3072/25000 (12%)]\tLoss: 0.133239\n",
      "Train Epoch: 12 [4096/25000 (16%)]\tLoss: 0.120755\n",
      "Train Epoch: 12 [5120/25000 (20%)]\tLoss: 0.134138\n",
      "Train Epoch: 12 [6144/25000 (24%)]\tLoss: 0.151842\n",
      "Train Epoch: 12 [7168/25000 (29%)]\tLoss: 0.178257\n",
      "Train Epoch: 12 [8192/25000 (33%)]\tLoss: 0.145694\n",
      "Train Epoch: 12 [9216/25000 (37%)]\tLoss: 0.134231\n",
      "Train Epoch: 12 [10240/25000 (41%)]\tLoss: 0.135041\n",
      "Train Epoch: 12 [11264/25000 (45%)]\tLoss: 0.137567\n",
      "Train Epoch: 12 [12288/25000 (49%)]\tLoss: 0.171397\n",
      "Train Epoch: 12 [13312/25000 (53%)]\tLoss: 0.180589\n",
      "Train Epoch: 12 [14336/25000 (57%)]\tLoss: 0.145865\n",
      "Train Epoch: 12 [15360/25000 (61%)]\tLoss: 0.146564\n",
      "Train Epoch: 12 [16384/25000 (65%)]\tLoss: 0.165845\n",
      "Train Epoch: 12 [17408/25000 (69%)]\tLoss: 0.140936\n",
      "Train Epoch: 12 [18432/25000 (73%)]\tLoss: 0.177379\n",
      "Train Epoch: 12 [19456/25000 (78%)]\tLoss: 0.146978\n",
      "Train Epoch: 12 [20480/25000 (82%)]\tLoss: 0.143765\n",
      "Train Epoch: 12 [21504/25000 (86%)]\tLoss: 0.128511\n",
      "Train Epoch: 12 [22528/25000 (90%)]\tLoss: 0.157040\n",
      "Train Epoch: 12 [23552/25000 (94%)]\tLoss: 0.134321\n",
      "Train Epoch: 12 [20352/25000 (98%)]\tLoss: 0.138794\n",
      "Train Epoch: 13 [0/25000 (0%)]\tLoss: 0.138035\n",
      "Train Epoch: 13 [1024/25000 (4%)]\tLoss: 0.150804\n",
      "Train Epoch: 13 [2048/25000 (8%)]\tLoss: 0.135284\n",
      "Train Epoch: 13 [3072/25000 (12%)]\tLoss: 0.126311\n",
      "Train Epoch: 13 [4096/25000 (16%)]\tLoss: 0.138033\n",
      "Train Epoch: 13 [5120/25000 (20%)]\tLoss: 0.125549\n",
      "Train Epoch: 13 [6144/25000 (24%)]\tLoss: 0.148723\n",
      "Train Epoch: 13 [7168/25000 (29%)]\tLoss: 0.168853\n",
      "Train Epoch: 13 [8192/25000 (33%)]\tLoss: 0.168067\n",
      "Train Epoch: 13 [9216/25000 (37%)]\tLoss: 0.152248\n",
      "Train Epoch: 13 [10240/25000 (41%)]\tLoss: 0.122800\n",
      "Train Epoch: 13 [11264/25000 (45%)]\tLoss: 0.128417\n",
      "Train Epoch: 13 [12288/25000 (49%)]\tLoss: 0.170162\n",
      "Train Epoch: 13 [13312/25000 (53%)]\tLoss: 0.158621\n",
      "Train Epoch: 13 [14336/25000 (57%)]\tLoss: 0.130548\n",
      "Train Epoch: 13 [15360/25000 (61%)]\tLoss: 0.119504\n",
      "Train Epoch: 13 [16384/25000 (65%)]\tLoss: 0.140760\n",
      "Train Epoch: 13 [17408/25000 (69%)]\tLoss: 0.167262\n",
      "Train Epoch: 13 [18432/25000 (73%)]\tLoss: 0.130175\n",
      "Train Epoch: 13 [19456/25000 (78%)]\tLoss: 0.165226\n",
      "Train Epoch: 13 [20480/25000 (82%)]\tLoss: 0.129223\n",
      "Train Epoch: 13 [21504/25000 (86%)]\tLoss: 0.139961\n",
      "Train Epoch: 13 [22528/25000 (90%)]\tLoss: 0.156779\n",
      "Train Epoch: 13 [23552/25000 (94%)]\tLoss: 0.142408\n",
      "Train Epoch: 13 [20352/25000 (98%)]\tLoss: 0.114541\n",
      "Train Epoch: 14 [0/25000 (0%)]\tLoss: 0.142908\n",
      "Train Epoch: 14 [1024/25000 (4%)]\tLoss: 0.134594\n",
      "Train Epoch: 14 [2048/25000 (8%)]\tLoss: 0.129197\n",
      "Train Epoch: 14 [3072/25000 (12%)]\tLoss: 0.117973\n",
      "Train Epoch: 14 [4096/25000 (16%)]\tLoss: 0.118072\n",
      "Train Epoch: 14 [5120/25000 (20%)]\tLoss: 0.140113\n",
      "Train Epoch: 14 [6144/25000 (24%)]\tLoss: 0.112989\n",
      "Train Epoch: 14 [7168/25000 (29%)]\tLoss: 0.132441\n",
      "Train Epoch: 14 [8192/25000 (33%)]\tLoss: 0.118966\n",
      "Train Epoch: 14 [9216/25000 (37%)]\tLoss: 0.118178\n",
      "Train Epoch: 14 [10240/25000 (41%)]\tLoss: 0.134036\n",
      "Train Epoch: 14 [11264/25000 (45%)]\tLoss: 0.138631\n",
      "Train Epoch: 14 [12288/25000 (49%)]\tLoss: 0.151975\n",
      "Train Epoch: 14 [13312/25000 (53%)]\tLoss: 0.122195\n",
      "Train Epoch: 14 [14336/25000 (57%)]\tLoss: 0.114718\n",
      "Train Epoch: 14 [15360/25000 (61%)]\tLoss: 0.114295\n",
      "Train Epoch: 14 [16384/25000 (65%)]\tLoss: 0.141285\n",
      "Train Epoch: 14 [17408/25000 (69%)]\tLoss: 0.144200\n",
      "Train Epoch: 14 [18432/25000 (73%)]\tLoss: 0.135158\n",
      "Train Epoch: 14 [19456/25000 (78%)]\tLoss: 0.120146\n",
      "Train Epoch: 14 [20480/25000 (82%)]\tLoss: 0.114953\n",
      "Train Epoch: 14 [21504/25000 (86%)]\tLoss: 0.125505\n",
      "Train Epoch: 14 [22528/25000 (90%)]\tLoss: 0.131116\n",
      "Train Epoch: 14 [23552/25000 (94%)]\tLoss: 0.139717\n",
      "Train Epoch: 14 [20352/25000 (98%)]\tLoss: 0.138583\n",
      "Train Epoch: 15 [0/25000 (0%)]\tLoss: 0.114988\n",
      "Train Epoch: 15 [1024/25000 (4%)]\tLoss: 0.116196\n",
      "Train Epoch: 15 [2048/25000 (8%)]\tLoss: 0.124692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [3072/25000 (12%)]\tLoss: 0.114743\n",
      "Train Epoch: 15 [4096/25000 (16%)]\tLoss: 0.145561\n",
      "Train Epoch: 15 [5120/25000 (20%)]\tLoss: 0.110855\n",
      "Train Epoch: 15 [6144/25000 (24%)]\tLoss: 0.112541\n",
      "Train Epoch: 15 [7168/25000 (29%)]\tLoss: 0.110916\n",
      "Train Epoch: 15 [8192/25000 (33%)]\tLoss: 0.099931\n",
      "Train Epoch: 15 [9216/25000 (37%)]\tLoss: 0.124577\n",
      "Train Epoch: 15 [10240/25000 (41%)]\tLoss: 0.112432\n",
      "Train Epoch: 15 [11264/25000 (45%)]\tLoss: 0.130316\n",
      "Train Epoch: 15 [12288/25000 (49%)]\tLoss: 0.112499\n",
      "Train Epoch: 15 [13312/25000 (53%)]\tLoss: 0.116408\n",
      "Train Epoch: 15 [14336/25000 (57%)]\tLoss: 0.091822\n",
      "Train Epoch: 15 [15360/25000 (61%)]\tLoss: 0.107791\n",
      "Train Epoch: 15 [16384/25000 (65%)]\tLoss: 0.094705\n",
      "Train Epoch: 15 [17408/25000 (69%)]\tLoss: 0.112029\n",
      "Train Epoch: 15 [18432/25000 (73%)]\tLoss: 0.123025\n",
      "Train Epoch: 15 [19456/25000 (78%)]\tLoss: 0.099876\n",
      "Train Epoch: 15 [20480/25000 (82%)]\tLoss: 0.106074\n",
      "Train Epoch: 15 [21504/25000 (86%)]\tLoss: 0.147845\n",
      "Train Epoch: 15 [22528/25000 (90%)]\tLoss: 0.112342\n",
      "Train Epoch: 15 [23552/25000 (94%)]\tLoss: 0.144431\n",
      "Train Epoch: 15 [20352/25000 (98%)]\tLoss: 0.103101\n",
      "Train Epoch: 16 [0/25000 (0%)]\tLoss: 0.105276\n",
      "Train Epoch: 16 [1024/25000 (4%)]\tLoss: 0.114761\n",
      "Train Epoch: 16 [2048/25000 (8%)]\tLoss: 0.116021\n",
      "Train Epoch: 16 [3072/25000 (12%)]\tLoss: 0.123857\n",
      "Train Epoch: 16 [4096/25000 (16%)]\tLoss: 0.123830\n",
      "Train Epoch: 16 [5120/25000 (20%)]\tLoss: 0.097584\n",
      "Train Epoch: 16 [6144/25000 (24%)]\tLoss: 0.112437\n",
      "Train Epoch: 16 [7168/25000 (29%)]\tLoss: 0.107035\n",
      "Train Epoch: 16 [8192/25000 (33%)]\tLoss: 0.075310\n",
      "Train Epoch: 16 [9216/25000 (37%)]\tLoss: 0.121387\n",
      "Train Epoch: 16 [10240/25000 (41%)]\tLoss: 0.133743\n",
      "Train Epoch: 16 [11264/25000 (45%)]\tLoss: 0.092146\n",
      "Train Epoch: 16 [12288/25000 (49%)]\tLoss: 0.124010\n",
      "Train Epoch: 16 [13312/25000 (53%)]\tLoss: 0.112615\n",
      "Train Epoch: 16 [14336/25000 (57%)]\tLoss: 0.094333\n",
      "Train Epoch: 16 [15360/25000 (61%)]\tLoss: 0.135929\n",
      "Train Epoch: 16 [16384/25000 (65%)]\tLoss: 0.119254\n",
      "Train Epoch: 16 [17408/25000 (69%)]\tLoss: 0.095539\n",
      "Train Epoch: 16 [18432/25000 (73%)]\tLoss: 0.095691\n",
      "Train Epoch: 16 [19456/25000 (78%)]\tLoss: 0.107582\n",
      "Train Epoch: 16 [20480/25000 (82%)]\tLoss: 0.096540\n",
      "Train Epoch: 16 [21504/25000 (86%)]\tLoss: 0.133633\n",
      "Train Epoch: 16 [22528/25000 (90%)]\tLoss: 0.109471\n",
      "Train Epoch: 16 [23552/25000 (94%)]\tLoss: 0.115636\n",
      "Train Epoch: 16 [20352/25000 (98%)]\tLoss: 0.089481\n",
      "Train Epoch: 17 [0/25000 (0%)]\tLoss: 0.109258\n",
      "Train Epoch: 17 [1024/25000 (4%)]\tLoss: 0.104406\n",
      "Train Epoch: 17 [2048/25000 (8%)]\tLoss: 0.107267\n",
      "Train Epoch: 17 [3072/25000 (12%)]\tLoss: 0.105874\n",
      "Train Epoch: 17 [4096/25000 (16%)]\tLoss: 0.089997\n",
      "Train Epoch: 17 [5120/25000 (20%)]\tLoss: 0.091349\n",
      "Train Epoch: 17 [6144/25000 (24%)]\tLoss: 0.083200\n",
      "Train Epoch: 17 [7168/25000 (29%)]\tLoss: 0.113048\n",
      "Train Epoch: 17 [8192/25000 (33%)]\tLoss: 0.091812\n",
      "Train Epoch: 17 [9216/25000 (37%)]\tLoss: 0.097293\n",
      "Train Epoch: 17 [10240/25000 (41%)]\tLoss: 0.098125\n",
      "Train Epoch: 17 [11264/25000 (45%)]\tLoss: 0.111315\n",
      "Train Epoch: 17 [12288/25000 (49%)]\tLoss: 0.075894\n",
      "Train Epoch: 17 [13312/25000 (53%)]\tLoss: 0.078847\n",
      "Train Epoch: 17 [14336/25000 (57%)]\tLoss: 0.123134\n",
      "Train Epoch: 17 [15360/25000 (61%)]\tLoss: 0.127883\n",
      "Train Epoch: 17 [16384/25000 (65%)]\tLoss: 0.094591\n",
      "Train Epoch: 17 [17408/25000 (69%)]\tLoss: 0.102168\n",
      "Train Epoch: 17 [18432/25000 (73%)]\tLoss: 0.114403\n",
      "Train Epoch: 17 [19456/25000 (78%)]\tLoss: 0.083114\n",
      "Train Epoch: 17 [20480/25000 (82%)]\tLoss: 0.123799\n",
      "Train Epoch: 17 [21504/25000 (86%)]\tLoss: 0.126833\n",
      "Train Epoch: 17 [22528/25000 (90%)]\tLoss: 0.115649\n",
      "Train Epoch: 17 [23552/25000 (94%)]\tLoss: 0.109332\n",
      "Train Epoch: 17 [20352/25000 (98%)]\tLoss: 0.109801\n",
      "Train Epoch: 18 [0/25000 (0%)]\tLoss: 0.098770\n",
      "Train Epoch: 18 [1024/25000 (4%)]\tLoss: 0.091394\n",
      "Train Epoch: 18 [2048/25000 (8%)]\tLoss: 0.093410\n",
      "Train Epoch: 18 [3072/25000 (12%)]\tLoss: 0.095117\n",
      "Train Epoch: 18 [4096/25000 (16%)]\tLoss: 0.087804\n",
      "Train Epoch: 18 [5120/25000 (20%)]\tLoss: 0.108091\n",
      "Train Epoch: 18 [6144/25000 (24%)]\tLoss: 0.099567\n",
      "Train Epoch: 18 [7168/25000 (29%)]\tLoss: 0.101690\n",
      "Train Epoch: 18 [8192/25000 (33%)]\tLoss: 0.090925\n",
      "Train Epoch: 18 [9216/25000 (37%)]\tLoss: 0.084202\n",
      "Train Epoch: 18 [10240/25000 (41%)]\tLoss: 0.105641\n",
      "Train Epoch: 18 [11264/25000 (45%)]\tLoss: 0.080280\n",
      "Train Epoch: 18 [12288/25000 (49%)]\tLoss: 0.100207\n",
      "Train Epoch: 18 [13312/25000 (53%)]\tLoss: 0.093299\n",
      "Train Epoch: 18 [14336/25000 (57%)]\tLoss: 0.101612\n",
      "Train Epoch: 18 [15360/25000 (61%)]\tLoss: 0.089255\n",
      "Train Epoch: 18 [16384/25000 (65%)]\tLoss: 0.095393\n",
      "Train Epoch: 18 [17408/25000 (69%)]\tLoss: 0.087088\n",
      "Train Epoch: 18 [18432/25000 (73%)]\tLoss: 0.089552\n",
      "Train Epoch: 18 [19456/25000 (78%)]\tLoss: 0.103395\n",
      "Train Epoch: 18 [20480/25000 (82%)]\tLoss: 0.096969\n",
      "Train Epoch: 18 [21504/25000 (86%)]\tLoss: 0.097380\n",
      "Train Epoch: 18 [22528/25000 (90%)]\tLoss: 0.103041\n",
      "Train Epoch: 18 [23552/25000 (94%)]\tLoss: 0.109193\n",
      "Train Epoch: 18 [20352/25000 (98%)]\tLoss: 0.085792\n",
      "Train Epoch: 19 [0/25000 (0%)]\tLoss: 0.093193\n",
      "Train Epoch: 19 [1024/25000 (4%)]\tLoss: 0.087029\n",
      "Train Epoch: 19 [2048/25000 (8%)]\tLoss: 0.094290\n",
      "Train Epoch: 19 [3072/25000 (12%)]\tLoss: 0.093651\n",
      "Train Epoch: 19 [4096/25000 (16%)]\tLoss: 0.096356\n",
      "Train Epoch: 19 [5120/25000 (20%)]\tLoss: 0.108333\n",
      "Train Epoch: 19 [6144/25000 (24%)]\tLoss: 0.104427\n",
      "Train Epoch: 19 [7168/25000 (29%)]\tLoss: 0.093996\n",
      "Train Epoch: 19 [8192/25000 (33%)]\tLoss: 0.097042\n",
      "Train Epoch: 19 [9216/25000 (37%)]\tLoss: 0.113494\n",
      "Train Epoch: 19 [10240/25000 (41%)]\tLoss: 0.108316\n",
      "Train Epoch: 19 [11264/25000 (45%)]\tLoss: 0.085438\n",
      "Train Epoch: 19 [12288/25000 (49%)]\tLoss: 0.090646\n",
      "Train Epoch: 19 [13312/25000 (53%)]\tLoss: 0.077589\n",
      "Train Epoch: 19 [14336/25000 (57%)]\tLoss: 0.082948\n",
      "Train Epoch: 19 [15360/25000 (61%)]\tLoss: 0.090613\n",
      "Train Epoch: 19 [16384/25000 (65%)]\tLoss: 0.085407\n",
      "Train Epoch: 19 [17408/25000 (69%)]\tLoss: 0.085894\n",
      "Train Epoch: 19 [18432/25000 (73%)]\tLoss: 0.110127\n",
      "Train Epoch: 19 [19456/25000 (78%)]\tLoss: 0.102055\n",
      "Train Epoch: 19 [20480/25000 (82%)]\tLoss: 0.097202\n",
      "Train Epoch: 19 [21504/25000 (86%)]\tLoss: 0.086513\n",
      "Train Epoch: 19 [22528/25000 (90%)]\tLoss: 0.086973\n",
      "Train Epoch: 19 [23552/25000 (94%)]\tLoss: 0.077279\n",
      "Train Epoch: 19 [20352/25000 (98%)]\tLoss: 0.114185\n",
      "Train Epoch: 20 [0/25000 (0%)]\tLoss: 0.086139\n",
      "Train Epoch: 20 [1024/25000 (4%)]\tLoss: 0.068494\n",
      "Train Epoch: 20 [2048/25000 (8%)]\tLoss: 0.086577\n",
      "Train Epoch: 20 [3072/25000 (12%)]\tLoss: 0.087736\n",
      "Train Epoch: 20 [4096/25000 (16%)]\tLoss: 0.064938\n",
      "Train Epoch: 20 [5120/25000 (20%)]\tLoss: 0.080950\n",
      "Train Epoch: 20 [6144/25000 (24%)]\tLoss: 0.075813\n",
      "Train Epoch: 20 [7168/25000 (29%)]\tLoss: 0.080967\n",
      "Train Epoch: 20 [8192/25000 (33%)]\tLoss: 0.103819\n",
      "Train Epoch: 20 [9216/25000 (37%)]\tLoss: 0.088811\n",
      "Train Epoch: 20 [10240/25000 (41%)]\tLoss: 0.069259\n",
      "Train Epoch: 20 [11264/25000 (45%)]\tLoss: 0.098323\n",
      "Train Epoch: 20 [12288/25000 (49%)]\tLoss: 0.093748\n",
      "Train Epoch: 20 [13312/25000 (53%)]\tLoss: 0.078565\n",
      "Train Epoch: 20 [14336/25000 (57%)]\tLoss: 0.077758\n",
      "Train Epoch: 20 [15360/25000 (61%)]\tLoss: 0.075042\n",
      "Train Epoch: 20 [16384/25000 (65%)]\tLoss: 0.074752\n",
      "Train Epoch: 20 [17408/25000 (69%)]\tLoss: 0.089472\n",
      "Train Epoch: 20 [18432/25000 (73%)]\tLoss: 0.072601\n",
      "Train Epoch: 20 [19456/25000 (78%)]\tLoss: 0.088010\n",
      "Train Epoch: 20 [20480/25000 (82%)]\tLoss: 0.071233\n",
      "Train Epoch: 20 [21504/25000 (86%)]\tLoss: 0.100868\n",
      "Train Epoch: 20 [22528/25000 (90%)]\tLoss: 0.073768\n",
      "Train Epoch: 20 [23552/25000 (94%)]\tLoss: 0.091029\n",
      "Train Epoch: 20 [20352/25000 (98%)]\tLoss: 0.092508\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2:40 ночи, начал в 20:00 : мне, наконец, удалось понять, откуда на Windows Memory Error, как перенести всё на GPU и в какой нормальной форме работать с PyTorch (потому что вариантов миллиард, а адекватный только этот). Окошко снизу для референса - показатели по одной из первых попыток (специально их здесь оставлю).\n",
    "\n",
    "### 3:30 ночи, при очередном рестарте ядра для очистки памяти окошко я случайно грохнул. Простите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "x, y = Variable(torch.from_numpy(vectorize_sequences(test_data))), Variable(torch.from_numpy(np.asarray(test_labels).astype('float32')), requires_grad=False)\n",
    "pred = model(x.type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.argmax(pred.data.cpu().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.asarray(test_labels), final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Точность оставляет желать лучшего. Возможно, связано с тем, что я не разобрался, как сделать два линейных слоя: понятно для CNN, но для Linear не получается. Но это поправимо. И - учитывая все безумные трудности - оно оно хотя бы работает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приступаем к многоклассовой классификации. Буду делать её в этом же файле, но, если куда-то не так нажму, у меня начнётся истерика: для того, чтобы сделать здесь же, нужно перезагружать ядро для очистки видеопамяти - при неудачных попытках делая это ОЧЕНЬ часто - и можно случайно убить все output'ы Jupyter'а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDatasetTrain(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X_train = vectorize_sequences(data)\n",
    "        self.y_train = to_categorical(labels)\n",
    "    def __getitem__(self, index):\n",
    "        item = self.X_train[index]\n",
    "        label = self.y_train[index]\n",
    "        return item, label\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = ReutersDatasetTrain(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dset_train,\n",
    "                          batch_size = 512,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 0,\n",
    "                          pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построим и обучим сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,46)\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x)\n",
    "    \n",
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).type(dtype), Variable(target).type(dtype)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n",
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8982 (0%)]\tLoss: 0.105386\n",
      "Train Epoch: 1 [1024/8982 (11%)]\tLoss: 0.087039\n",
      "Train Epoch: 1 [2048/8982 (22%)]\tLoss: 0.067745\n",
      "Train Epoch: 1 [3072/8982 (33%)]\tLoss: 0.056700\n",
      "Train Epoch: 1 [4096/8982 (44%)]\tLoss: 0.050416\n",
      "Train Epoch: 1 [5120/8982 (56%)]\tLoss: 0.048322\n",
      "Train Epoch: 1 [6144/8982 (67%)]\tLoss: 0.045251\n",
      "Train Epoch: 1 [7168/8982 (78%)]\tLoss: 0.044747\n",
      "Train Epoch: 1 [8192/8982 (89%)]\tLoss: 0.039976\n",
      "Train Epoch: 2 [0/8982 (0%)]\tLoss: 0.035131\n",
      "Train Epoch: 2 [1024/8982 (11%)]\tLoss: 0.038614\n",
      "Train Epoch: 2 [2048/8982 (22%)]\tLoss: 0.037272\n",
      "Train Epoch: 2 [3072/8982 (33%)]\tLoss: 0.033387\n",
      "Train Epoch: 2 [4096/8982 (44%)]\tLoss: 0.033156\n",
      "Train Epoch: 2 [5120/8982 (56%)]\tLoss: 0.034456\n",
      "Train Epoch: 2 [6144/8982 (67%)]\tLoss: 0.034082\n",
      "Train Epoch: 2 [7168/8982 (78%)]\tLoss: 0.033741\n",
      "Train Epoch: 2 [8192/8982 (89%)]\tLoss: 0.031818\n",
      "Train Epoch: 3 [0/8982 (0%)]\tLoss: 0.026049\n",
      "Train Epoch: 3 [1024/8982 (11%)]\tLoss: 0.027091\n",
      "Train Epoch: 3 [2048/8982 (22%)]\tLoss: 0.030835\n",
      "Train Epoch: 3 [3072/8982 (33%)]\tLoss: 0.024596\n",
      "Train Epoch: 3 [4096/8982 (44%)]\tLoss: 0.026748\n",
      "Train Epoch: 3 [5120/8982 (56%)]\tLoss: 0.025417\n",
      "Train Epoch: 3 [6144/8982 (67%)]\tLoss: 0.024836\n",
      "Train Epoch: 3 [7168/8982 (78%)]\tLoss: 0.027196\n",
      "Train Epoch: 3 [8192/8982 (89%)]\tLoss: 0.023083\n",
      "Train Epoch: 4 [0/8982 (0%)]\tLoss: 0.022684\n",
      "Train Epoch: 4 [1024/8982 (11%)]\tLoss: 0.020749\n",
      "Train Epoch: 4 [2048/8982 (22%)]\tLoss: 0.022453\n",
      "Train Epoch: 4 [3072/8982 (33%)]\tLoss: 0.020453\n",
      "Train Epoch: 4 [4096/8982 (44%)]\tLoss: 0.021736\n",
      "Train Epoch: 4 [5120/8982 (56%)]\tLoss: 0.020194\n",
      "Train Epoch: 4 [6144/8982 (67%)]\tLoss: 0.024034\n",
      "Train Epoch: 4 [7168/8982 (78%)]\tLoss: 0.021675\n",
      "Train Epoch: 4 [8192/8982 (89%)]\tLoss: 0.019242\n",
      "Train Epoch: 5 [0/8982 (0%)]\tLoss: 0.015381\n",
      "Train Epoch: 5 [1024/8982 (11%)]\tLoss: 0.018554\n",
      "Train Epoch: 5 [2048/8982 (22%)]\tLoss: 0.018447\n",
      "Train Epoch: 5 [3072/8982 (33%)]\tLoss: 0.015004\n",
      "Train Epoch: 5 [4096/8982 (44%)]\tLoss: 0.018505\n",
      "Train Epoch: 5 [5120/8982 (56%)]\tLoss: 0.015298\n",
      "Train Epoch: 5 [6144/8982 (67%)]\tLoss: 0.016071\n",
      "Train Epoch: 5 [7168/8982 (78%)]\tLoss: 0.017370\n",
      "Train Epoch: 5 [8192/8982 (89%)]\tLoss: 0.015087\n",
      "Train Epoch: 6 [0/8982 (0%)]\tLoss: 0.015767\n",
      "Train Epoch: 6 [1024/8982 (11%)]\tLoss: 0.013372\n",
      "Train Epoch: 6 [2048/8982 (22%)]\tLoss: 0.014310\n",
      "Train Epoch: 6 [3072/8982 (33%)]\tLoss: 0.012716\n",
      "Train Epoch: 6 [4096/8982 (44%)]\tLoss: 0.014810\n",
      "Train Epoch: 6 [5120/8982 (56%)]\tLoss: 0.013339\n",
      "Train Epoch: 6 [6144/8982 (67%)]\tLoss: 0.015047\n",
      "Train Epoch: 6 [7168/8982 (78%)]\tLoss: 0.013258\n",
      "Train Epoch: 6 [8192/8982 (89%)]\tLoss: 0.016057\n",
      "Train Epoch: 7 [0/8982 (0%)]\tLoss: 0.011874\n",
      "Train Epoch: 7 [1024/8982 (11%)]\tLoss: 0.012104\n",
      "Train Epoch: 7 [2048/8982 (22%)]\tLoss: 0.011313\n",
      "Train Epoch: 7 [3072/8982 (33%)]\tLoss: 0.011002\n",
      "Train Epoch: 7 [4096/8982 (44%)]\tLoss: 0.010741\n",
      "Train Epoch: 7 [5120/8982 (56%)]\tLoss: 0.010441\n",
      "Train Epoch: 7 [6144/8982 (67%)]\tLoss: 0.014428\n",
      "Train Epoch: 7 [7168/8982 (78%)]\tLoss: 0.010414\n",
      "Train Epoch: 7 [8192/8982 (89%)]\tLoss: 0.013385\n",
      "Train Epoch: 8 [0/8982 (0%)]\tLoss: 0.012200\n",
      "Train Epoch: 8 [1024/8982 (11%)]\tLoss: 0.010636\n",
      "Train Epoch: 8 [2048/8982 (22%)]\tLoss: 0.008548\n",
      "Train Epoch: 8 [3072/8982 (33%)]\tLoss: 0.009706\n",
      "Train Epoch: 8 [4096/8982 (44%)]\tLoss: 0.010459\n",
      "Train Epoch: 8 [5120/8982 (56%)]\tLoss: 0.008565\n",
      "Train Epoch: 8 [6144/8982 (67%)]\tLoss: 0.009174\n",
      "Train Epoch: 8 [7168/8982 (78%)]\tLoss: 0.008677\n",
      "Train Epoch: 8 [8192/8982 (89%)]\tLoss: 0.010823\n",
      "Train Epoch: 9 [0/8982 (0%)]\tLoss: 0.008942\n",
      "Train Epoch: 9 [1024/8982 (11%)]\tLoss: 0.008142\n",
      "Train Epoch: 9 [2048/8982 (22%)]\tLoss: 0.008423\n",
      "Train Epoch: 9 [3072/8982 (33%)]\tLoss: 0.009794\n",
      "Train Epoch: 9 [4096/8982 (44%)]\tLoss: 0.008454\n",
      "Train Epoch: 9 [5120/8982 (56%)]\tLoss: 0.007601\n",
      "Train Epoch: 9 [6144/8982 (67%)]\tLoss: 0.008534\n",
      "Train Epoch: 9 [7168/8982 (78%)]\tLoss: 0.008071\n",
      "Train Epoch: 9 [8192/8982 (89%)]\tLoss: 0.007872\n",
      "Train Epoch: 10 [0/8982 (0%)]\tLoss: 0.005803\n",
      "Train Epoch: 10 [1024/8982 (11%)]\tLoss: 0.008675\n",
      "Train Epoch: 10 [2048/8982 (22%)]\tLoss: 0.007650\n",
      "Train Epoch: 10 [3072/8982 (33%)]\tLoss: 0.007987\n",
      "Train Epoch: 10 [4096/8982 (44%)]\tLoss: 0.008867\n",
      "Train Epoch: 10 [5120/8982 (56%)]\tLoss: 0.006006\n",
      "Train Epoch: 10 [6144/8982 (67%)]\tLoss: 0.008271\n",
      "Train Epoch: 10 [7168/8982 (78%)]\tLoss: 0.006751\n",
      "Train Epoch: 10 [8192/8982 (89%)]\tLoss: 0.007837\n",
      "Train Epoch: 11 [0/8982 (0%)]\tLoss: 0.007205\n",
      "Train Epoch: 11 [1024/8982 (11%)]\tLoss: 0.006644\n",
      "Train Epoch: 11 [2048/8982 (22%)]\tLoss: 0.007277\n",
      "Train Epoch: 11 [3072/8982 (33%)]\tLoss: 0.006387\n",
      "Train Epoch: 11 [4096/8982 (44%)]\tLoss: 0.007301\n",
      "Train Epoch: 11 [5120/8982 (56%)]\tLoss: 0.006479\n",
      "Train Epoch: 11 [6144/8982 (67%)]\tLoss: 0.006533\n",
      "Train Epoch: 11 [7168/8982 (78%)]\tLoss: 0.007396\n",
      "Train Epoch: 11 [8192/8982 (89%)]\tLoss: 0.007593\n",
      "Train Epoch: 12 [0/8982 (0%)]\tLoss: 0.005986\n",
      "Train Epoch: 12 [1024/8982 (11%)]\tLoss: 0.005658\n",
      "Train Epoch: 12 [2048/8982 (22%)]\tLoss: 0.006322\n",
      "Train Epoch: 12 [3072/8982 (33%)]\tLoss: 0.004993\n",
      "Train Epoch: 12 [4096/8982 (44%)]\tLoss: 0.004734\n",
      "Train Epoch: 12 [5120/8982 (56%)]\tLoss: 0.006580\n",
      "Train Epoch: 12 [6144/8982 (67%)]\tLoss: 0.005018\n",
      "Train Epoch: 12 [7168/8982 (78%)]\tLoss: 0.005591\n",
      "Train Epoch: 12 [8192/8982 (89%)]\tLoss: 0.006432\n",
      "Train Epoch: 13 [0/8982 (0%)]\tLoss: 0.004304\n",
      "Train Epoch: 13 [1024/8982 (11%)]\tLoss: 0.004937\n",
      "Train Epoch: 13 [2048/8982 (22%)]\tLoss: 0.005149\n",
      "Train Epoch: 13 [3072/8982 (33%)]\tLoss: 0.006071\n",
      "Train Epoch: 13 [4096/8982 (44%)]\tLoss: 0.005449\n",
      "Train Epoch: 13 [5120/8982 (56%)]\tLoss: 0.005765\n",
      "Train Epoch: 13 [6144/8982 (67%)]\tLoss: 0.005768\n",
      "Train Epoch: 13 [7168/8982 (78%)]\tLoss: 0.007304\n",
      "Train Epoch: 13 [8192/8982 (89%)]\tLoss: 0.005639\n",
      "Train Epoch: 14 [0/8982 (0%)]\tLoss: 0.004764\n",
      "Train Epoch: 14 [1024/8982 (11%)]\tLoss: 0.005071\n",
      "Train Epoch: 14 [2048/8982 (22%)]\tLoss: 0.004927\n",
      "Train Epoch: 14 [3072/8982 (33%)]\tLoss: 0.005125\n",
      "Train Epoch: 14 [4096/8982 (44%)]\tLoss: 0.004727\n",
      "Train Epoch: 14 [5120/8982 (56%)]\tLoss: 0.006390\n",
      "Train Epoch: 14 [6144/8982 (67%)]\tLoss: 0.005826\n",
      "Train Epoch: 14 [7168/8982 (78%)]\tLoss: 0.004751\n",
      "Train Epoch: 14 [8192/8982 (89%)]\tLoss: 0.005195\n",
      "Train Epoch: 15 [0/8982 (0%)]\tLoss: 0.004849\n",
      "Train Epoch: 15 [1024/8982 (11%)]\tLoss: 0.004320\n",
      "Train Epoch: 15 [2048/8982 (22%)]\tLoss: 0.004043\n",
      "Train Epoch: 15 [3072/8982 (33%)]\tLoss: 0.005210\n",
      "Train Epoch: 15 [4096/8982 (44%)]\tLoss: 0.003582\n",
      "Train Epoch: 15 [5120/8982 (56%)]\tLoss: 0.005740\n",
      "Train Epoch: 15 [6144/8982 (67%)]\tLoss: 0.007260\n",
      "Train Epoch: 15 [7168/8982 (78%)]\tLoss: 0.004989\n",
      "Train Epoch: 15 [8192/8982 (89%)]\tLoss: 0.005523\n",
      "Train Epoch: 16 [0/8982 (0%)]\tLoss: 0.004367\n",
      "Train Epoch: 16 [1024/8982 (11%)]\tLoss: 0.004306\n",
      "Train Epoch: 16 [2048/8982 (22%)]\tLoss: 0.005291\n",
      "Train Epoch: 16 [3072/8982 (33%)]\tLoss: 0.003766\n",
      "Train Epoch: 16 [4096/8982 (44%)]\tLoss: 0.002895\n",
      "Train Epoch: 16 [5120/8982 (56%)]\tLoss: 0.004846\n",
      "Train Epoch: 16 [6144/8982 (67%)]\tLoss: 0.003280\n",
      "Train Epoch: 16 [7168/8982 (78%)]\tLoss: 0.005201\n",
      "Train Epoch: 16 [8192/8982 (89%)]\tLoss: 0.006369\n",
      "Train Epoch: 17 [0/8982 (0%)]\tLoss: 0.003263\n",
      "Train Epoch: 17 [1024/8982 (11%)]\tLoss: 0.004444\n",
      "Train Epoch: 17 [2048/8982 (22%)]\tLoss: 0.005117\n",
      "Train Epoch: 17 [3072/8982 (33%)]\tLoss: 0.004422\n",
      "Train Epoch: 17 [4096/8982 (44%)]\tLoss: 0.004528\n",
      "Train Epoch: 17 [5120/8982 (56%)]\tLoss: 0.004336\n",
      "Train Epoch: 17 [6144/8982 (67%)]\tLoss: 0.004020\n",
      "Train Epoch: 17 [7168/8982 (78%)]\tLoss: 0.004757\n",
      "Train Epoch: 17 [8192/8982 (89%)]\tLoss: 0.004960\n",
      "Train Epoch: 18 [0/8982 (0%)]\tLoss: 0.002878\n",
      "Train Epoch: 18 [1024/8982 (11%)]\tLoss: 0.004110\n",
      "Train Epoch: 18 [2048/8982 (22%)]\tLoss: 0.003612\n",
      "Train Epoch: 18 [3072/8982 (33%)]\tLoss: 0.004077\n",
      "Train Epoch: 18 [4096/8982 (44%)]\tLoss: 0.003267\n",
      "Train Epoch: 18 [5120/8982 (56%)]\tLoss: 0.003575\n",
      "Train Epoch: 18 [6144/8982 (67%)]\tLoss: 0.003926\n",
      "Train Epoch: 18 [7168/8982 (78%)]\tLoss: 0.004844\n",
      "Train Epoch: 18 [8192/8982 (89%)]\tLoss: 0.004181\n",
      "Train Epoch: 19 [0/8982 (0%)]\tLoss: 0.004021\n",
      "Train Epoch: 19 [1024/8982 (11%)]\tLoss: 0.002612\n",
      "Train Epoch: 19 [2048/8982 (22%)]\tLoss: 0.004620\n",
      "Train Epoch: 19 [3072/8982 (33%)]\tLoss: 0.004242\n",
      "Train Epoch: 19 [4096/8982 (44%)]\tLoss: 0.003755\n",
      "Train Epoch: 19 [5120/8982 (56%)]\tLoss: 0.003533\n",
      "Train Epoch: 19 [6144/8982 (67%)]\tLoss: 0.004505\n",
      "Train Epoch: 19 [7168/8982 (78%)]\tLoss: 0.004928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [8192/8982 (89%)]\tLoss: 0.003431\n",
      "Train Epoch: 20 [0/8982 (0%)]\tLoss: 0.005509\n",
      "Train Epoch: 20 [1024/8982 (11%)]\tLoss: 0.004932\n",
      "Train Epoch: 20 [2048/8982 (22%)]\tLoss: 0.003663\n",
      "Train Epoch: 20 [3072/8982 (33%)]\tLoss: 0.003058\n",
      "Train Epoch: 20 [4096/8982 (44%)]\tLoss: 0.003761\n",
      "Train Epoch: 20 [5120/8982 (56%)]\tLoss: 0.003590\n",
      "Train Epoch: 20 [6144/8982 (67%)]\tLoss: 0.004769\n",
      "Train Epoch: 20 [7168/8982 (78%)]\tLoss: 0.004531\n",
      "Train Epoch: 20 [8192/8982 (89%)]\tLoss: 0.004113\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "x, y = Variable(torch.from_numpy(vectorize_sequences(test_data))), Variable(torch.from_numpy(np.asarray(test_labels).astype('float32')), requires_grad=False)\n",
    "pred = model(x.type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7960819234194123"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = np.argmax(pred.data.cpu().numpy(), axis=1)\n",
    "accuracy_score(np.asarray(test_labels), final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression, yay !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим модель и отчаянно попытаемся внедрить K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(train_data.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = 0.01)\n",
    "criterion = nn.MSELoss()# Mean Squared Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = Variable(data).type(dtype), Variable(target).type(dtype)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        if target.dim() != output.dim():\n",
    "            output = output.squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDatasetTrain(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X_train = data\n",
    "        self.y_train = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = self.X_train[index]\n",
    "        label = self.y_train[index]\n",
    "        return item, label\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 10\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    dset_train = HousingDatasetTrain(partial_train_data, partial_train_targets)\n",
    "    train_loader = DataLoader(dset_train,\n",
    "                          batch_size = 1,\n",
    "                          shuffle = False,\n",
    "                          num_workers = 0,\n",
    "                          pin_memory = True)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train(epoch, train_loader)\n",
    "    predicted = model.forward(Variable(torch.from_numpy(val_data).type(dtype))).cpu().data.numpy()\n",
    "    pred = np.swapaxes(predicted, 1, 0)\n",
    "    sub = np.subtract(val_targets, pred)\n",
    "    formae = np.absolute(sub[0])\n",
    "    mae = np.sum(formae)/formae.size\n",
    "    all_scores.append(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6070554766324485,\n",
       " 2.275290794183712,\n",
       " 2.338240768885848,\n",
       " 2.262471595613083,\n",
       " 2.250446368680142]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.346701000799047"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На десяти эпохах (взятых для экономии времени) в итоге достигнут лучший результат, чем на 100, и даже на 500 эпохах в оригинале. За сим (ибо 6 часов утра :D) и откланиваюсь."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
