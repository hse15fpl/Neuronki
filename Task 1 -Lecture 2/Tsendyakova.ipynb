{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKzbRANeVNBW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3 : Getting started with Neural Networks\n",
    "\n",
    "We’ll apply what you’ve learned to three new problems covering the three most common use cases of neural networks: binary classification, multiclass classification, and scalar regression.\n",
    "\n",
    "We’ll dive into three introductory examples of how to use neural networks to address real problems:\n",
    "\n",
    "* Classifying movie reviews as positive or negative (binary classification)\n",
    "* Classifying news wires by topic (multiclass classification)\n",
    "* Estimating the price of a house, given real-estate data (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHuc00aNVNBY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anatomy of a neural network\n",
    "\n",
    "(From previous lecture) training a neural network revolves around the following objects:\n",
    "\n",
    "* **Layers**, which are combined into a network (or model)\n",
    "* The **input data** and corresponding **targets**\n",
    "* The **loss function**, which defines the feedback signal used for learning\n",
    "* The **optimizer**, which determines how learning proceeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTQ1-yTpVNBZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can visualize their interaction:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig01.jpg)\n",
    "\n",
    "The network, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the network’s predictions match what was expected. The optimizer uses this loss value to update the network’s weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ux6lyIUGVNBb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "...Let’s take a closer look at layers, networks, loss functions, and optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE2fuOHCVNBb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Layers: the building blocks of deep learning\n",
    " \n",
    " The fundamental data structure in neural networks is the **layer**. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. \n",
    " \n",
    "Some layers are stateless, but more frequently layers have a state: the layer’s weights, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSmI3-PxVNBd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape ```(samples, features)```, is often processed by **densely connected layers**, also called **fully connected** or **dense layers** (the ```Dense``` class in Keras). Sequence data, stored in 3D tensors of shape ```(samples, timesteps, features)```, is typically processed by **recurrent** layers such as an **LSTM layer**. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (```Conv2D```).\n",
    "\n",
    "You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by frameworks like Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlViv-JKVNBe",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1661,
     "status": "ok",
     "timestamp": 1541848737073,
     "user": {
      "displayName": "Лингвисты 4 курс 15ФПЛ",
      "photoUrl": "",
      "userId": "07722014738513937459"
     },
     "user_tz": -120
    },
    "id": "Dnv0zp2zVNBf",
    "outputId": "3727017a-cd8d-4813-fdeb-13a8e82fbc22",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "layer = layers.Dense(32, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCkE-A-vVNBj",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We’re creating a layer that will only accept as input 2D tensors where the first dimension is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be accepted). This layer will return a tensor where the first dimension has been transformed to be 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq2HTr1hVNBl",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don’t have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjChb65-VNBm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "model.add(layers.Dense(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yi93Px6kVNBq",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The second layer didn’t receive an input shape argument—instead, it automatically inferred its input shape as being the output shape of the layer that came before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ciJlNu1fVNBr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Models: networks of layers\n",
    "\n",
    "A deep-learning model is a directed, acyclic graph of layers. The most common instance is a linear stack of layers, mapping a single input to a single output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EU9tgtQPVNBs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But as you move forward, you’ll be exposed to a much broader variety of network topologies. Some common ones include the following:\n",
    "\n",
    "* Two-branch networks\n",
    "* Multihead networks\n",
    "* Inception blocks\n",
    "\n",
    "The topology of a network defines a hypothesis space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixJlQV4JVNBt",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We defined *machine learning* as *searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.* \n",
    "\n",
    "By choosing a network topology, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG_6epPdVNBv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Picking the right network architecture is more an art than a science; and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tLlWX3nVNBw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Loss functions and optimizers: keys to configuring the learning process\n",
    "\n",
    "Once the network architecture is defined, you still have to choose two more things:\n",
    "\n",
    "* **Loss function (objective function)** — The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "* **Optimizer** — Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaXHG5Q2VNBy",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network that has multiple outputs may have multiple loss functions (one per output). But the gradient-descent process must be based on a single scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0ynLnK1VNB1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Choosing the right objective function for the right problem is extremely important**: your network will take any shortcut it can, to minimize the loss; so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69NBIAEvVNB4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine a stupid, omnipotent AI trained via SGD, with this poorly chosen objective function: “maximizing the average well-being of all humans alive.” To make its job easier, this AI might choose to kill all humans except a few and focus on the well-being of the remaining ones—because average well-being isn’t affected by how many humans are left. That might not be what you intended! Just remember that all neural networks you build will be just as ruthless in lowering their loss function—so choose the objective wisely, or you’ll have to face unintended side effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aH6i6HcYVNB6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. For instance, you’ll use **binary crossentropy** for a two-class classification problem, **categorical crossentropy** for a many-class classification problem, **mean-squared error** for a regression problem, **connectionist temporal classification (CTC)** for a sequence-learning problem, and so on. \n",
    "\n",
    "Only when you’re working on truly new research problems will you have to develop your own objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExOMklVtVNB7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Keras\n",
    "\n",
    "We'll use Keras Framework  (https://keras.io). Keras is a deep-learning framework for Python that provides a convenient way to define and train almost any kind of deep-learning model. Keras was initially developed for researchers, with the aim of enabling fast experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu_1PR9NVNB8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keras has the following key features:\n",
    "\n",
    "* It allows the same code to run seamlessly on CPU or GPU.\n",
    "* It has a user-friendly API that makes it easy to quickly prototype deep-learning models.\n",
    "* It has built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n",
    "* It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. This means Keras is appropriate for building essentially any deep-learning model, from a generative adversarial network to a neural Turing machine.\n",
    "\n",
    "It’s compatible with any version of Python from 2.7 to 3.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULTzjA38VNB9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Developing with Keras: a quick overview\n",
    "\n",
    "The typical Keras workflow looks just like that example:\n",
    "\n",
    "1. Define your training data: input tensors and target tensors.\n",
    "* Define a network of layers (or **model**) that maps your inputs to your targets.\n",
    "* Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.\n",
    "* Iterate on your training data by calling the ```fit()``` method of your model.\n",
    "\n",
    "There are two ways to define a model: using the ```Sequential``` class (only for linear stacks of layers, which is the most common network architecture by far) or the **functional API** (for directed acyclic graphs of layers, which lets you build completely arbitrary architectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsJeS-xfVNB-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here’s a two-layer model defined using the Sequential class (note that we’re passing the expected shape of the input data to the first layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4P9w3clVNB_",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xf6JYTVVNCB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And here’s the same model defined using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJrhKkJgVNCD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3jZeCj3VNCF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With the functional API, you’re manipulating the data tensors that the model processes and applying layers to this tensor as if they were functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wELzZDP3VNCH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once your model architecture is defined, it doesn’t matter whether you used a Sequential model or the functional API. All of the following steps are the same.\n",
    "\n",
    "The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here’s an example with a single loss function, which is by far the most common case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_s4hasJVNCI",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVlhh77PVNCK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "error",
     "timestamp": 1541848749744,
     "user": {
      "displayName": "Лингвисты 4 курс 15ФПЛ",
      "photoUrl": "",
      "userId": "07722014738513937459"
     },
     "user_tz": -120
    },
    "id": "h9swIbMRVNCM",
    "outputId": "be171e13-4cbc-4cd4-e3f9-b1326dcd172c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c17b2397d844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2hw0GpEVNCP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying movie reviews: a binary classification example\n",
    "\n",
    "Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig7a_DgPVNCQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  The IMDB dataset\n",
    "\n",
    "You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPHT8vuuVNCR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why use separate training and test sets?\n",
    "* Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen.\n",
    "* Risk of just memoirizing a mapping between training samples and their targets, which would be useless for the task of predicting targes for data the model never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mbh7JghSVNCS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (**warning** first time running = 80 Mb of data downloading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csi7pqmnVNCV",
    "outputId": "4f5f5504-fd10-4955-e01a-f58ba6b1bca4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 61s 3us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-Ib74BGVNCd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The argument num_words=10000 means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_330RMvqVNCd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5UDb-E6VNCe",
    "outputId": "6910c210-05b4-4c55-db2a-349764b96905",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFaaCrdeVNCi",
    "outputId": "848ba41c-ea73-4fa9-a13a-04d44255f954",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKtSyA3rVNCm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2KW7w1xVNCn",
    "outputId": "bf74f71c-c876-499e-e7ed-0709cc68ba92",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byOt6L9FVNCq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For kicks, here’s how you can quickly decode one of these reviews back to English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJ1NPRzGVNCr",
    "outputId": "235c3241-8549-4bbe-ed36-7e0211ba67dd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 3s 2us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOPxQMdFVNCx",
    "outputId": "abafcfe3-4852-428e-d52e-55b99f2e2ffd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xicv7Xn4VNC4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparing the data\n",
    "\n",
    "You can’t just feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:\n",
    "\n",
    "* Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the ```Embedding``` layer, which we’ll cover in detail later).\n",
    "* One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence ```[3, 5]``` into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a ```Dense``` layer, capable of handling floating-point vector data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZLd_cwZVNC6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s go with the latter solution to vectorize the data, which you’ll do manually for maximum clarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzF3cL3wVNC7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xUCSZYJVNC-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here’s what the samples look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLT4Wu2KVNC-",
    "outputId": "c2e82aec-93ca-40d8-93c8-73d30cbc859d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xs9R_Yc4VNDE",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You should also vectorize your labels, which is straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0aaOu6pVNDH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zyXZHpjPVNDL",
    "outputId": "bfd263a6-0953-4f4c-ea84-5923d9d0ecb2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5mPcnugVNDP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now the data is ready to be fed into a neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBkLzmOwVNDQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building your network\n",
    "\n",
    "The input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (```Dense```) layers with relu activations: ```Dense(16, activation='relu')```.\n",
    "\n",
    "The argument being passed to each ```Dense``` layer (16) is the number of hidden units of the layer. A hidden unit is a dimension in the representation space of the layer. \n",
    "\n",
    "From previous lecture each such Dense layer with a relu activation implements the following chain of tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88qQlyOvVNDS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-18b539df7e9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'relu' is not defined"
     ]
    }
   ],
   "source": [
    "output = relu(dot(W, input) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9M74rMmVNDX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Having 16 hidden units means the weight matrix W will have shape (input_dimension, 16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrgbApG2VNDY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two key architecture decisions to be made about such a stack of Dense layers:\n",
    "\n",
    "* How many layers to use\n",
    "* How many hidden units to choose for each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzNfSDpyVNDa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Later, you’ll learn formal principles to guide you in making these choices. Now we choose following architecture:\n",
    "\n",
    "* Two intermediate layers with 16 hidden units each\n",
    "* A third layer that will output the scalar prediction regarding the sentiment of the current review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89GR23GWVNDb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The intermediate layers will use **relu** as their activation function, and the final layer will use a **sigmoid** activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be positive). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIF2wzb0VNDg",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A relu (rectified linear unit) is a function meant to zero out negative values\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig04_alt.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HS8ItrSsVNDh",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A sigmoid** “squashes” arbitrary values into the [0, 1] interval, outputting something that can be interpreted as a probability.\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig05_alt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3RoaNumVNDj",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "See how the network looks like. And here’s the Keras implementation, similar to the MNIST example you saw previously.\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig06.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vWjLKPyVNDl",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8B6A7d4gVNDp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### WHAT ARE ACTIVATION FUNCTIONS, AND WHY ARE THEY NECESSARY?\n",
    "\n",
    "Without an activation function like ```relu``` (also called a **non-linearity**), the ```Dense``` layer would consist of two linear operations—a dot product and an addition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-ZFI0UxVNDr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-613c5963416f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dot' is not defined"
     ]
    }
   ],
   "source": [
    "output = dot(W, input) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shp6un4ZVNDt",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So the layer could only learn **linear transformations** (*affine transformations*) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.\n",
    "\n",
    "In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. ```relu``` is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: ```prelu```, ```elu```, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwMtliMhVNDu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, you need to choose a loss function and an optimizer. \n",
    "\n",
    "Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the ```binary_crossentropy``` loss. It isn’t the only viable choice: you could use, for instance, ```mean_squared_error```. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. **Crossentropy** is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O41ItN_ZVNDv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here’s the step where you configure the model with the rmsprop optimizer and the binary_crossentropy loss function. Note that you’ll also monitor accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSaqnbedVNDv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcF6qD4RVNDy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You’re passing your optimizer, loss function, and metrics as strings, which is possible because ```rmsprop```, ```binary_crossentropy```, and ```accuracy``` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer class instance as the optimizer argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sE-_mgQqVNDz",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnB5--I1VND0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The latter can be done by passing function objects as the loss and/or metrics arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qx6JcQm3VND1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5eBZqg8wVND3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validating your approach\n",
    "\n",
    "In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX-zMsvRVND4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6D2DJgXSVND8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You’ll now train the model for 5 epochs (5 iterations over all samples in the ```x_train``` and ```y_train``` tensors), in mini-batches of 512 samples. At the same time, you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by passing the validation data as the ```validation_data``` argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBXmUBSzVND9",
    "outputId": "41e051b5-bd59-49a6-9eeb-a974c7ff83be",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 5s 347us/step - loss: 0.5090 - acc: 0.7813 - val_loss: 0.3794 - val_acc: 0.8686\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 4s 294us/step - loss: 0.3006 - acc: 0.9049 - val_loss: 0.3003 - val_acc: 0.8893\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 4s 296us/step - loss: 0.2180 - acc: 0.9285 - val_loss: 0.3084 - val_acc: 0.8716\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.1751 - acc: 0.9435 - val_loss: 0.2836 - val_acc: 0.8837\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.1426 - acc: 0.9541 - val_loss: 0.2842 - val_acc: 0.8872\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.1149 - acc: 0.9652 - val_loss: 0.3149 - val_acc: 0.8775\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.0978 - acc: 0.9708 - val_loss: 0.3128 - val_acc: 0.8844\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.0806 - acc: 0.9765 - val_loss: 0.3862 - val_acc: 0.8650\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 4s 287us/step - loss: 0.0660 - acc: 0.9821 - val_loss: 0.3638 - val_acc: 0.8779\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 4s 278us/step - loss: 0.0559 - acc: 0.9853 - val_loss: 0.3845 - val_acc: 0.8791\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 4s 276us/step - loss: 0.0439 - acc: 0.9891 - val_loss: 0.4154 - val_acc: 0.8777\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.0380 - acc: 0.9919 - val_loss: 0.4530 - val_acc: 0.8686\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.0298 - acc: 0.9930 - val_loss: 0.4712 - val_acc: 0.8732\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.0247 - acc: 0.9945 - val_loss: 0.5049 - val_acc: 0.8723\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.0191 - acc: 0.9964 - val_loss: 0.5326 - val_acc: 0.8702\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.0164 - acc: 0.9968 - val_loss: 0.5655 - val_acc: 0.8696\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.0125 - acc: 0.9981 - val_loss: 0.5976 - val_acc: 0.8672\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.0109 - acc: 0.9980 - val_loss: 0.6287 - val_acc: 0.8672\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.7393 - val_acc: 0.8538\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.0049 - acc: 0.9998 - val_loss: 0.6837 - val_acc: 0.8669\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-w7AKjDzVND_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On CPU, this will take less than 2 seconds per epoch—training is over in 10 seconds. At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
    "\n",
    "Note that the call to ```model.fit()``` returns a History object. This object has a member history, which is a dictionary containing data about everything that happened during training. Let’s look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EInzcWUVNEB",
    "outputId": "cb64a7fe-2072-44fc-faa4-7b15820910f0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwYskvGwVNEE",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The dictionary contains four entries: one per metric that was being monitored during training and during validation. In the following two listing, let’s use Matplotlib to plot the training and validation loss side by side, as well as the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTWRnF1eVNEF",
    "outputId": "5ac9a395-188d-4c72-f710-27b7c0f95f6f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(history_dict['acc']) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_Q-7ko5VNEI",
    "outputId": "68e87f19-1cd9-4805-d654-1c8b82dda5b6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjOe_8V-VNES",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running gradient-descent optimization—the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_O7qdsD_VNET",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is overfitting: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lcf9ec8sVNET",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s train a new network from scratch for four epochs`and then evaluate it on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzhq4XzCVNEV",
    "outputId": "e86c8688-9d00-4646-94f0-3b770372d215",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 251us/step - loss: 0.4748 - acc: 0.8215\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 197us/step - loss: 0.2670 - acc: 0.9096\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 196us/step - loss: 0.1989 - acc: 0.9288\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 200us/step - loss: 0.1680 - acc: 0.9397\n",
      "25000/25000 [==============================] - 8s 304us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8UInNYJVNEY",
    "outputId": "b3ba7099-55cf-45be-ff78-2efee920ccd0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.323026871919632, 0.87304]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLOy6qi-VNEc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using a trained network to generate predictions on new data\n",
    "\n",
    "After having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the ```predict``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07GarCBOVNEd",
    "outputId": "e99bc173-4778-464d-d987-6241a28ad705",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13864213],\n",
       "       [0.9997187 ],\n",
       "       [0.30075884],\n",
       "       ...,\n",
       "       [0.07540161],\n",
       "       [0.04213535],\n",
       "       [0.47333336]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3m94ORa1VNEh",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can see, the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpjdgsS6VNEi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### !!!!!!!!! Home Tasks !!!!!!!!!!!\n",
    "\n",
    "\n",
    "The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:\n",
    "\n",
    "* You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "* Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.\n",
    "* Try using the ```mse``` loss function instead of ```binary_crossentropy```.\n",
    "* Try using the ```tanh``` activation (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 5s 214us/step - loss: 0.4598 - acc: 0.8118\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 197us/step - loss: 0.2533 - acc: 0.9102\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 197us/step - loss: 0.1965 - acc: 0.9281\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 198us/step - loss: 0.1645 - acc: 0.9396\n",
      "25000/25000 [==============================] - 5s 208us/step\n"
     ]
    }
   ],
   "source": [
    "#You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3374016036748886, 0.87132]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В данном случае  loss увеличился на 1 сотую, поэтому лучше оставить 1 хиддэн лэйер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 237us/step - loss: 0.4429 - acc: 0.8149\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 216us/step - loss: 0.2490 - acc: 0.9105\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 215us/step - loss: 0.1915 - acc: 0.9305\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 214us/step - loss: 0.1609 - acc: 0.9415\n",
      "25000/25000 [==============================] - 6s 235us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3479495885372162, 0.86656]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 222us/step - loss: 0.4434 - acc: 0.8308\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 200us/step - loss: 0.2406 - acc: 0.9130\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 202us/step - loss: 0.1882 - acc: 0.9293\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 202us/step - loss: 0.1583 - acc: 0.9424\n",
      "25000/25000 [==============================] - 5s 211us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3069522474861145, 0.88104]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 7s 267us/step - loss: 0.4234 - acc: 0.8174\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 6s 231us/step - loss: 0.2429 - acc: 0.9083\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 6s 224us/step - loss: 0.1881 - acc: 0.9300\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 218us/step - loss: 0.1557 - acc: 0.9433\n",
      "25000/25000 [==============================] - 6s 238us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31982400312900544, 0.87848]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самый лучший результат [0.3069522474861145, 0.88104] получился для одного хиддэн слоя с 65 хиддэн юнитс и первый слой с 16 хиддэн юнитс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 257us/step - loss: 0.1511 - acc: 0.8159\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 199us/step - loss: 0.0752 - acc: 0.9101\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 196us/step - loss: 0.0556 - acc: 0.9316\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 199us/step - loss: 0.0447 - acc: 0.9472\n",
      "25000/25000 [==============================] - 5s 212us/step\n"
     ]
    }
   ],
   "source": [
    "#Try using the mse loss function instead of binary_crossentropy.\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08833851920366287, 0.87996]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ви хэв вэри литтл лосс, бат стилл биг эккьюраси, which is good, but it can be connected with MSE respresentation and the way how it counts loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 236us/step - loss: 0.4027 - acc: 0.8314\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 6s 236us/step - loss: 0.2187 - acc: 0.9154\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 6s 243us/step - loss: 0.1708 - acc: 0.9367\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 208us/step - loss: 0.1499 - acc: 0.9440\n",
      "25000/25000 [==============================] - 5s 197us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3363238091373444, 0.87524]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu.\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 229us/step - loss: 0.1310 - acc: 0.8228\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 216us/step - loss: 0.0649 - acc: 0.9157\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 193us/step - loss: 0.0498 - acc: 0.9372\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 194us/step - loss: 0.0425 - acc: 0.9468\n",
      "25000/25000 [==============================] - 6s 230us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09677285987064242, 0.87468]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the tahn activation results are worse \n",
    "### relu: [0.31982400312900544, 0.87848]\n",
    "### tahn:  [0.3363238091373444, 0.87524]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYW-OcjYVNEi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapping Up\n",
    "\n",
    "* You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it—as tensors—into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpPhW9BZVNEj",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Stacks of ```Dense``` layers with ```relu``` activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IifOG3PmVNEm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In a binary classification problem (two output classes), your network should end with a ```Dense``` layer with one unit and a ```sigmoid``` activation: the output of your network should be a scalar between 0 and 1, encoding a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG5HxwasVNEn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With such a scalar sigmoid output on a binary classification problem, the loss function you should use is ```binary_crossentropy```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKrXzf_VVNEn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The ```rmsprop``` optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXN5J2BBVNEo",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwbRQ95OVNEp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying newswires: a multiclass classification example\n",
    "\n",
    "What happens when you have more than two classes?\n",
    "\n",
    "Now, we’ll build a network to classify *Reuters newswires* into 46 mutually exclusive topics. Because you have many classes, this problem is an instance of **multiclass classification**; and because each data point should be classified into only one category, the problem is more specifically an instance of **single-label, multiclass classification**. If each data point could belong to multiple categories (in this case, topics), you’d be facing a **multilabel**, **multiclass classification** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2LU1K56VNEp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Reuters dataset\n",
    "\n",
    "You’ll work with the Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s take a look.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbo7GK_bVNEr",
    "outputId": "aeee09ad-680b-470b-c7ff-938c1af2d010",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJIIyiZtVNEt",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with the IMDB dataset, the argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data.\n",
    "\n",
    "You have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZ04Xa6BVNEv",
    "outputId": "4af7acd1-1b88-46a0-b8c6-6124ca6486f0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 8982\n",
      "Test examples: 2246\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: {}\".format(len(train_data)))\n",
    "print(\"Test examples: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jsFfFVrVNEx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag85QQr3VNEy",
    "outputId": "85904ad8-020d-434b-9483-27362e57a72a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55M6O56VVNE0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here’s how you can decode it back to words, in case you’re curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96_oqKVmVNE1",
    "outputId": "2f7c6875-0a02-49e8-9698-9f78ba8739fa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 2s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "    train_data[0]])\n",
    "decoded_newswire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzN4ql05VNE3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The label associated with an example is an integer between 0 and 45—a topic index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2cYUBztVNE4",
    "outputId": "0bafdc38-9d1d-4953-f9bc-c4a11dfdca05",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3HtOySJVNE6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparing the data\n",
    "\n",
    "You can vectorize the data with the exact same code as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSmC2IbQVNE7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIj7BlUTVNFB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To vectorize the labels, there are two possibilities: you can cast the label list as an integer tensor, or you can use one-hot encoding. One-hot encoding is a widely used format for categorical data, also called **categorical encoding**.\n",
    "\n",
    "In our case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C32RSozpVNFC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjCYTvDsVNFF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that there is a built-in way to do this in Keras, which you’ve already seen in action in the MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wblPwb9GVNFF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4q9EtZ-VNFI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building your network\n",
    "\n",
    "This topic-classification problem looks similar to the previous movie-review classification problem: in both cases, you’re trying to classify short snippets of text. But there is a new constraint here: the number of output classes has gone from 2 to 46. The dimensionality of the output space is much larger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNCNVKhnVNFK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a stack of ```Dense``` layers like that you’ve been using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an information bottleneck. In the previous example, you used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\n",
    "\n",
    "For this reason you’ll use larger layers. Let’s go with 64 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6l81vNZVNFK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZbC3rNgVNFP",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* You end the network with a ```Dense``` layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a ```softmax``` activation. You saw this pattern in the MNIST example. It means the network will output a **probability distribution** over the 46 different output classes—for every input sample, the network will produce a 46-dimensional output vector, where ```output[i]``` is the probability that the sample belongs to class ```i```. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is ```categorical_crossentropy```. It measures the distance between two probability distributions: here, between the probability distribution output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, you train the network to output something as close as possible to the true labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybW-gluhVNFQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YF9Uw5S3VNFS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validating your approach\n",
    "\n",
    "Let’s set apart 1,000 samples in the training data to use as a validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqV7ztSrVNFT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrL99_xzVNFW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let’s train the network for 20 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZE9ZsMeVNFX",
    "outputId": "6efb45ee-cf3a-4891-ee18-dce50c9b2229",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 499us/step - loss: 2.5241 - acc: 0.4977 - val_loss: 1.7183 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 295us/step - loss: 1.4443 - acc: 0.6889 - val_loss: 1.3496 - val_acc: 0.7090\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 306us/step - loss: 1.0993 - acc: 0.7641 - val_loss: 1.1745 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 294us/step - loss: 0.8729 - acc: 0.8157 - val_loss: 1.0842 - val_acc: 0.7580\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 290us/step - loss: 0.7061 - acc: 0.8492 - val_loss: 0.9869 - val_acc: 0.7830\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 0.5696 - acc: 0.8790 - val_loss: 0.9418 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 283us/step - loss: 0.4626 - acc: 0.9034 - val_loss: 0.9092 - val_acc: 0.8030\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 271us/step - loss: 0.3728 - acc: 0.9221 - val_loss: 0.9330 - val_acc: 0.7910\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 293us/step - loss: 0.3052 - acc: 0.9315 - val_loss: 0.8901 - val_acc: 0.8060\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 295us/step - loss: 0.2547 - acc: 0.9415 - val_loss: 0.9053 - val_acc: 0.8140\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 287us/step - loss: 0.2191 - acc: 0.9473 - val_loss: 0.9172 - val_acc: 0.8110\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 0.1877 - acc: 0.9513 - val_loss: 0.9061 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 274us/step - loss: 0.1704 - acc: 0.9523 - val_loss: 0.9317 - val_acc: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 289us/step - loss: 0.1534 - acc: 0.9555 - val_loss: 0.9633 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 277us/step - loss: 0.1393 - acc: 0.9562 - val_loss: 0.9672 - val_acc: 0.8130\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 0.1315 - acc: 0.9559 - val_loss: 1.0246 - val_acc: 0.8030\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 281us/step - loss: 0.1221 - acc: 0.9575 - val_loss: 1.0278 - val_acc: 0.7990\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 274us/step - loss: 0.1199 - acc: 0.9570 - val_loss: 1.0403 - val_acc: 0.8040\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 276us/step - loss: 0.1140 - acc: 0.9593 - val_loss: 1.0962 - val_acc: 0.7940\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 300us/step - loss: 0.1113 - acc: 0.9595 - val_loss: 1.0677 - val_acc: 0.7980\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVnWojTbVNFb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, let’s display its loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J97BZfHsVNFb",
    "outputId": "cd433677-d8a1-439b-a315-f09f8c7ea68a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luvMjnbRVNFh",
    "outputId": "24d66a53-d1be-4fe9-f576-b7abf7c7b7b0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QA9ER1iHVNFk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When network begins overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXyG4WzWVNFk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s train a new network from scratch for nine epochs and then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZbiy4uGVNFn",
    "outputId": "056bf6ff-bfd3-4007-d78f-c3a8c316b547",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 3s 385us/step - loss: 2.5398 - acc: 0.5226 - val_loss: 1.6733 - val_acc: 0.6570\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s 281us/step - loss: 1.3712 - acc: 0.7121 - val_loss: 1.2758 - val_acc: 0.7210\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 3s 331us/step - loss: 1.0136 - acc: 0.7781 - val_loss: 1.1303 - val_acc: 0.7530\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 3s 314us/step - loss: 0.7976 - acc: 0.8251 - val_loss: 1.0539 - val_acc: 0.7590\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 2s 306us/step - loss: 0.6393 - acc: 0.8624 - val_loss: 0.9754 - val_acc: 0.7920\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 3s 363us/step - loss: 0.5124 - acc: 0.8923 - val_loss: 0.9102 - val_acc: 0.8140\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 2s 282us/step - loss: 0.4123 - acc: 0.9137 - val_loss: 0.8932 - val_acc: 0.8210\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 0.3354 - acc: 0.9288 - val_loss: 0.8732 - val_acc: 0.8260\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 2s 285us/step - loss: 0.2782 - acc: 0.9371 - val_loss: 0.9337 - val_acc: 0.8010\n",
      "2246/2246 [==============================] - 1s 353us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0222080261284818, 0.7756010686194165]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd_wtN71VNFs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%. But in this case it’s closer to 19%, so the results seem pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crefxt_JVNFu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating predictions on new data\n",
    "\n",
    "You can verify that the ```predict``` method of the model instance returns a probability distribution over all 46 topics. Let’s generate topic predictions for all of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSUkCrPBVNFv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generating predictions for new data\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QR8kdPHcVNFy",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each entry in predictions is a vector of length 46 and coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQL3jTi-VNFy",
    "outputId": "23fe0e10-f75f-477b-ee17-38fa6437167f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (46,)\n",
      "Sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('Shape: {}'.format(predictions[0].shape))\n",
    "print('Sum: {:.4f}'.format(np.sum(predictions[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjFuXtdYVNF0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The largest entry is the predicted class—the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBCA2KpvVNF0",
    "outputId": "31ab7dac-82f9-448c-f5ca-e31847401dd6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJmQKvwcVNF2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ8itCWYVNF2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vk3htBb5VNF4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The only thing this approach would change is the choice of the loss function. The loss function ```categorical_crossentropy```, expects the labels to follow a categorical encoding. With integer labels, you should use ```sparse_categorical_crossentropy```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O23Kp19RVNF4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uWGjlVfVNF7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This new loss function is still mathematically the same as ```categorical_crossentropy```; it just has a different interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zAEGIoxVNF8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The importance of having sufficiently large intermediate layers\n",
    "\n",
    "We mentioned earlier that because the final outputs are 46-dimensional, you should avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what happens when you introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensional: for example, 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bDlV0X-VNF8",
    "outputId": "4479fa6b-3a8d-49bf-cf2f-8a91a1e37f59",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 540us/step - loss: 3.1696 - acc: 0.2433 - val_loss: 2.6142 - val_acc: 0.2740\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 3s 437us/step - loss: 2.0606 - acc: 0.5461 - val_loss: 1.7033 - val_acc: 0.5860\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 4s 449us/step - loss: 1.5012 - acc: 0.6233 - val_loss: 1.5122 - val_acc: 0.6390\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 4s 453us/step - loss: 1.2887 - acc: 0.6907 - val_loss: 1.4123 - val_acc: 0.6770\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 4s 465us/step - loss: 1.1457 - acc: 0.7156 - val_loss: 1.3675 - val_acc: 0.6820\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 4s 455us/step - loss: 1.0403 - acc: 0.7308 - val_loss: 1.3422 - val_acc: 0.6950\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 4s 455us/step - loss: 0.9584 - acc: 0.7429 - val_loss: 1.3404 - val_acc: 0.7000\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 3s 432us/step - loss: 0.8890 - acc: 0.7534 - val_loss: 1.3359 - val_acc: 0.7020\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 4s 451us/step - loss: 0.8267 - acc: 0.7687 - val_loss: 1.3586 - val_acc: 0.7110\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 4s 443us/step - loss: 0.7663 - acc: 0.7898 - val_loss: 1.3749 - val_acc: 0.7070\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 434us/step - loss: 0.7129 - acc: 0.8058 - val_loss: 1.3987 - val_acc: 0.7150\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 4s 454us/step - loss: 0.6665 - acc: 0.8176 - val_loss: 1.4001 - val_acc: 0.7200\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 4s 451us/step - loss: 0.6241 - acc: 0.8294 - val_loss: 1.4448 - val_acc: 0.7120\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 4s 443us/step - loss: 0.5881 - acc: 0.8369 - val_loss: 1.4679 - val_acc: 0.7190\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 4s 460us/step - loss: 0.5537 - acc: 0.8452 - val_loss: 1.5174 - val_acc: 0.7170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 4s 482us/step - loss: 0.5228 - acc: 0.8544 - val_loss: 1.5604 - val_acc: 0.7200\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 4s 452us/step - loss: 0.4953 - acc: 0.8642 - val_loss: 1.5805 - val_acc: 0.7210\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 4s 472us/step - loss: 0.4694 - acc: 0.8787 - val_loss: 1.6405 - val_acc: 0.7170\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 4s 464us/step - loss: 0.4449 - acc: 0.8846 - val_loss: 1.6740 - val_acc: 0.7170\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 4s 473us/step - loss: 0.4284 - acc: 0.8885 - val_loss: 1.7236 - val_acc: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4fe77b70>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1sjh3_7VNF_",
    "outputId": "3a8a9c82-b1bf-4d95-bf4c-d9d1678a4ad3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 1s 364us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9613452537295442, 0.6878895814781835]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9bdZxXdVNGC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The network now peaks at ~69% validation accuracy, an 8% absolute drop. This drop is mostly due to the fact that you’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram most of the necessary information into these eight-dimensional representations, but not all of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwDjx8zzVNGE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further experiments (Hometask)\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
    "* You used two hidden layers. Now try using a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using larger or smaller layers: 32 units, 128 units, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 524us/step - loss: 2.2490 - acc: 0.5496 - val_loss: 1.4940 - val_acc: 0.6470\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 3s 348us/step - loss: 1.2452 - acc: 0.7215 - val_loss: 1.1812 - val_acc: 0.7370\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 3s 374us/step - loss: 0.9260 - acc: 0.8027 - val_loss: 1.0247 - val_acc: 0.7750\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 3s 355us/step - loss: 0.7109 - acc: 0.8517 - val_loss: 0.9411 - val_acc: 0.8000\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 3s 359us/step - loss: 0.5469 - acc: 0.8849 - val_loss: 0.9260 - val_acc: 0.8040\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 3s 353us/step - loss: 0.4298 - acc: 0.9112 - val_loss: 0.8898 - val_acc: 0.8190\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 3s 359us/step - loss: 0.3419 - acc: 0.9255 - val_loss: 0.9031 - val_acc: 0.8200\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 3s 351us/step - loss: 0.2799 - acc: 0.9346 - val_loss: 0.9247 - val_acc: 0.8120\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 3s 357us/step - loss: 0.2393 - acc: 0.9419 - val_loss: 0.9718 - val_acc: 0.8080\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 3s 344us/step - loss: 0.2055 - acc: 0.9471 - val_loss: 0.9334 - val_acc: 0.8180\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 349us/step - loss: 0.1826 - acc: 0.9511 - val_loss: 1.0170 - val_acc: 0.8030\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 3s 348us/step - loss: 0.1669 - acc: 0.9533 - val_loss: 0.9866 - val_acc: 0.8000\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 3s 349us/step - loss: 0.1513 - acc: 0.9551 - val_loss: 1.0644 - val_acc: 0.8030\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 3s 341us/step - loss: 0.1417 - acc: 0.9564 - val_loss: 1.0512 - val_acc: 0.8040\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 3s 340us/step - loss: 0.1349 - acc: 0.9555 - val_loss: 1.1430 - val_acc: 0.7890\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 3s 340us/step - loss: 0.1296 - acc: 0.9578 - val_loss: 1.1041 - val_acc: 0.7980\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 3s 328us/step - loss: 0.1221 - acc: 0.9565 - val_loss: 1.2019 - val_acc: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 3s 329us/step - loss: 0.1205 - acc: 0.9562 - val_loss: 1.1298 - val_acc: 0.7960\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 3s 341us/step - loss: 0.1182 - acc: 0.9564 - val_loss: 1.1790 - val_acc: 0.7950\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 3s 348us/step - loss: 0.1151 - acc: 0.9550 - val_loss: 1.1419 - val_acc: 0.7980\n",
      "2246/2246 [==============================] - 1s 256us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3926327466327806, 0.769813000890472]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 402us/step - loss: 2.4358 - acc: 0.5264 - val_loss: 1.6554 - val_acc: 0.6060\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 272us/step - loss: 1.4069 - acc: 0.6859 - val_loss: 1.3130 - val_acc: 0.7100\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 277us/step - loss: 1.1029 - acc: 0.7503 - val_loss: 1.1519 - val_acc: 0.7380\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 289us/step - loss: 0.9011 - acc: 0.8006 - val_loss: 1.0604 - val_acc: 0.7670\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 280us/step - loss: 0.7454 - acc: 0.8327 - val_loss: 1.0087 - val_acc: 0.7880\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 0.6224 - acc: 0.8548 - val_loss: 0.9810 - val_acc: 0.7950\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 267us/step - loss: 0.5207 - acc: 0.8770 - val_loss: 0.9727 - val_acc: 0.7880\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 269us/step - loss: 0.4379 - acc: 0.8985 - val_loss: 0.9466 - val_acc: 0.8060\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 280us/step - loss: 0.3721 - acc: 0.9146 - val_loss: 0.9496 - val_acc: 0.8030\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 283us/step - loss: 0.3180 - acc: 0.9295 - val_loss: 0.9470 - val_acc: 0.8120\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 283us/step - loss: 0.2758 - acc: 0.9370 - val_loss: 0.9651 - val_acc: 0.8030\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 287us/step - loss: 0.2413 - acc: 0.9435 - val_loss: 0.9855 - val_acc: 0.8050\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 265us/step - loss: 0.2167 - acc: 0.9479 - val_loss: 1.0121 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 0.1957 - acc: 0.9518 - val_loss: 1.0763 - val_acc: 0.7970\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 262us/step - loss: 0.1760 - acc: 0.9529 - val_loss: 1.0442 - val_acc: 0.8010\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 271us/step - loss: 0.1647 - acc: 0.9546 - val_loss: 1.0677 - val_acc: 0.8040\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 281us/step - loss: 0.1539 - acc: 0.9535 - val_loss: 1.0907 - val_acc: 0.8020\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 259us/step - loss: 0.1460 - acc: 0.9540 - val_loss: 1.1287 - val_acc: 0.7990\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 263us/step - loss: 0.1410 - acc: 0.9551 - val_loss: 1.1111 - val_acc: 0.7980\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 266us/step - loss: 0.1325 - acc: 0.9569 - val_loss: 1.1923 - val_acc: 0.7940\n",
      "2246/2246 [==============================] - 0s 222us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3931284160987671, 0.7698130009435482]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 hidden units make things worse, 32 hidden units are are optimal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You used two hidden layers. Now try using a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s 461us/step - loss: 2.2444 - acc: 0.6012 - val_loss: 1.4904 - val_acc: 0.6840\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 3s 375us/step - loss: 1.2027 - acc: 0.7502 - val_loss: 1.1401 - val_acc: 0.7490\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 3s 377us/step - loss: 0.8857 - acc: 0.8156 - val_loss: 1.0058 - val_acc: 0.7960\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 3s 352us/step - loss: 0.6868 - acc: 0.8573 - val_loss: 0.9289 - val_acc: 0.8100\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 3s 333us/step - loss: 0.5429 - acc: 0.8894 - val_loss: 0.8741 - val_acc: 0.8210\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 3s 344us/step - loss: 0.4346 - acc: 0.9092 - val_loss: 0.8422 - val_acc: 0.8260\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 3s 352us/step - loss: 0.3537 - acc: 0.9258 - val_loss: 0.8380 - val_acc: 0.8250\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 3s 339us/step - loss: 0.2947 - acc: 0.9344 - val_loss: 0.8279 - val_acc: 0.8320\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 3s 356us/step - loss: 0.2497 - acc: 0.9440 - val_loss: 0.8431 - val_acc: 0.8270\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 3s 344us/step - loss: 0.2145 - acc: 0.9484 - val_loss: 0.8584 - val_acc: 0.8250\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 342us/step - loss: 0.1886 - acc: 0.9508 - val_loss: 0.8469 - val_acc: 0.8300\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 3s 342us/step - loss: 0.1684 - acc: 0.9525 - val_loss: 0.8767 - val_acc: 0.8280\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 3s 343us/step - loss: 0.1520 - acc: 0.9538 - val_loss: 0.8875 - val_acc: 0.8280\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 3s 340us/step - loss: 0.1426 - acc: 0.9530 - val_loss: 0.9210 - val_acc: 0.8180\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 3s 344us/step - loss: 0.1323 - acc: 0.9565 - val_loss: 0.9363 - val_acc: 0.8150\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 3s 326us/step - loss: 0.1240 - acc: 0.9557 - val_loss: 0.9682 - val_acc: 0.8080\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 3s 324us/step - loss: 0.1197 - acc: 0.9563 - val_loss: 1.0045 - val_acc: 0.8060\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 3s 326us/step - loss: 0.1135 - acc: 0.9577 - val_loss: 1.0035 - val_acc: 0.8120\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 3s 323us/step - loss: 0.1101 - acc: 0.9570 - val_loss: 1.0154 - val_acc: 0.8110\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 3s 325us/step - loss: 0.1058 - acc: 0.9563 - val_loss: 1.0249 - val_acc: 0.8130\n",
      "2246/2246 [==============================] - 1s 257us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2035166953996368, 0.7902938557966204]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 389us/step - loss: 2.6493 - acc: 0.4842 - val_loss: 1.7456 - val_acc: 0.6080\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 281us/step - loss: 1.4941 - acc: 0.6627 - val_loss: 1.3849 - val_acc: 0.6780\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 285us/step - loss: 1.2062 - acc: 0.7184 - val_loss: 1.2594 - val_acc: 0.6970\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 296us/step - loss: 1.0070 - acc: 0.7602 - val_loss: 1.1601 - val_acc: 0.7430\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 0.8408 - acc: 0.8074 - val_loss: 1.1161 - val_acc: 0.7480\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 273us/step - loss: 0.6967 - acc: 0.8394 - val_loss: 1.0617 - val_acc: 0.7670\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 267us/step - loss: 0.5794 - acc: 0.8659 - val_loss: 1.1139 - val_acc: 0.7680\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 288us/step - loss: 0.4845 - acc: 0.8872 - val_loss: 1.0556 - val_acc: 0.7840\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 276us/step - loss: 0.4058 - acc: 0.9085 - val_loss: 1.0848 - val_acc: 0.7840\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 269us/step - loss: 0.3398 - acc: 0.9227 - val_loss: 1.0744 - val_acc: 0.7880\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 267us/step - loss: 0.2951 - acc: 0.9390 - val_loss: 1.1600 - val_acc: 0.7910 0.2988 - acc: 0.\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 258us/step - loss: 0.2585 - acc: 0.9436 - val_loss: 1.1587 - val_acc: 0.7780\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 259us/step - loss: 0.2324 - acc: 0.9451 - val_loss: 1.1940 - val_acc: 0.7950\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 261us/step - loss: 0.2064 - acc: 0.9503 - val_loss: 1.1965 - val_acc: 0.7910\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 271us/step - loss: 0.1929 - acc: 0.9511 - val_loss: 1.2937 - val_acc: 0.7840\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 273us/step - loss: 0.1790 - acc: 0.9533 - val_loss: 1.3508 - val_acc: 0.7800\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 271us/step - loss: 0.1728 - acc: 0.9539 - val_loss: 1.3450 - val_acc: 0.7790\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 241us/step - loss: 0.1566 - acc: 0.9554 - val_loss: 1.3130 - val_acc: 0.7790\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 269us/step - loss: 0.1554 - acc: 0.9567 - val_loss: 1.4342 - val_acc: 0.7620\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 274us/step - loss: 0.1503 - acc: 0.9559 - val_loss: 1.3478 - val_acc: 0.7840\n",
      "2246/2246 [==============================] - 0s 217us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.536920202914244, 0.7595725734639359]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More hidden layers cause the loss to increse, results with only one hidden layer are better \n",
    "#### One:  [1.2035166953996368, 0.7902938557966204]\n",
    "#### Three: [1.536920202914244, 0.7595725734639359]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7TO59PYKVNGE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapping up\n",
    "\n",
    "* If you’re trying to classify data points among N classes, your network should end with a ```Dense``` layer of size N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpuW92F9VNGF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In a single-label, multiclass classification problem, your network should end with a ```softmax``` activation so that it will output a probability distribution over the N output classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGpz8IskVNGF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the network and the true distribution of the targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUE5UlYaVNGG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* There are two ways to handle labels in multiclass classification:\n",
    "     * Encoding the labels via categorical encoding (also known as one-hot encoding) and using ```categorical_crossentropy``` as a loss function\n",
    "     \n",
    "     * Encoding the labels as integers and using the ```sparse_categorical_crossentropy``` loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjUviO9DVNGG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V426k-CSVNGH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting house prices: a regression example\n",
    "\n",
    "The two previous examples were considered classification problems, where the goal was to predict a single discrete label of an input data point. Another common type of machine-learning problem is regression, which consists of predicting a continuous value instead of a discrete label: for instance, predicting the temperature tomorrow, given meteorological data; or predicting the time that a software project will take to complete, given its specifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3QoxLMuVNGH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Boston Housing Price dataset\n",
    "\n",
    "\n",
    "You’ll attempt to predict the median price of homes in a given Boston suburb in the mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on. The dataset you’ll use has an interesting difference from the two previous examples. It has relatively few data points: only 506, split between 404 training samples and 102 test samples. And each feature in the input data (for example, the crime rate) has a different scale. For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others between 0 and 100, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ed9VqZDMVNGI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhKGVPgcVNGK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let’s look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5EWtzt7VNGK",
    "outputId": "9f0425c3-ae69-4426-9bde-2cf2a7143701",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (404, 13)\n",
      "Shape of test data: (102, 13)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train data: {}'.format(train_data.shape))\n",
    "print('Shape of test data: {}'.format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MF2GDPoTVNGM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The targets are the median values of owner-occupied homes, in thousands of dollars:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDLKtZ1RVNGM",
    "outputId": "16f69aac-68a6-4433-e5e4-a78d10c90e9e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4])"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVPvzozlVNGP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The prices are typically between \\$10,000 and \\$50,000. If that sounds cheap, remember that this was the mid-1970s, and these prices aren’t adjusted for inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_bAy0vQVNGQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Preparing the data\n",
    "\n",
    "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in Numpy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fK_8A2prVNGQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkZw4O__VNGS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the quantities used for normalizing the test data are computed using the training data. You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOPePtHXVNGT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building your network\n",
    "\n",
    "Because so few samples are available, you’ll use a very small network with two hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3-e7HqphVNGU",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlsUNyM6VNGW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The network ends with a single unit and no activation (it will be a linear layer). This is a typical setup for scalar regression (a regression where you’re trying to predict a single continuous value). Applying an activation function would constrain the range the output can take; for instance, if you applied a ```sigmoid``` activation function to the last layer, the network could only learn to predict values between 0 and 1. Here, because the last layer is purely linear, the network is free to learn to predict values in any range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVVvzqU3VNGW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that you compile the network with the ```mse``` loss function—**mean squared error**, the square of the difference between the predictions and the targets. This is a widely used loss function for regression problems.\n",
    "\n",
    "You’re also monitoring a new metric during training: **mean absolute error (MAE)**. It’s the absolute value of the difference between the predictions and the targets. For instance, an MAE of 0.5 on this problem would mean your predictions are off by \\$500 on average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-3dK5FNVNGX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validating your approach using K-fold validation\n",
    "\n",
    "To evaluate your network while you keep adjusting its parameters (such as the number of epochs used for training), you could split the data into a training set and a validation set, as you did in the previous examples. But because you have so few data points, the validation set would end up being very small (for instance, about 100 examples). As a consequence, the validation scores might change a lot depending on which data points you chose to use for validation and which you chose for training: the validation scores might have a high variance with regard to the validation split. This would prevent you from reliably evaluating your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_X14K0owVNGX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The best practice in such situations is to use **K-fold cross-validation**. It consists of splitting the available data into **K** partitions (typically K = 4 or 5), instantiating K identical models, and training each one on K – 1 partitions while evaluating on the remaining partition. The validation score for the model used is then the average of the K validation scores obtained. In terms of code, this is straightforward.\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig11_alt.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "is5VfNYAVNGX",
    "outputId": "72bb6bb8-dfb6-4faf-f8c1-ea7a39f71a52",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxHY4mzNVNGa",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Running this with num_epochs = 100 yields the following results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8Czl9QRVNGa",
    "outputId": "4caf749d-0ca4-44c2-e856-9b1e095d19c4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.08013649624173, 2.095389439327882, 2.96017064434467, 2.273717975262368]\n",
      "2.3523536387941624\n"
     ]
    }
   ],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcYdih1EVNGh",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The different runs do indeed show rather different validation scores, from 2.08 to 2.96. The average (3.0) is a much more reliable metric than any single score—that’s the entire point of K-fold cross-validation. In this case, you’re off by \\$3,000 on average, which is significant considering that the prices range from \\$10,000 to \\$50,000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6zoVvfuVNGl",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s try training the network a bit longer: 500 epochs. To keep a record of how well the model does at each epoch, you’ll modify the training loop to save the per-epoch validation score log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nw4fLehxVNGm",
    "outputId": "f3ee1e08-b401-40bc-b27a-7f39e29dedb0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M34R4GNGVNGp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can then compute the average of the per-epoch MAE scores for all folds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fcu0Qa-MVNGq",
    "outputId": "fb561ebf-e00c-4551-e8da-cd4117f434fa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tubb3u4rVNGt",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It may be a little difficult to see the plot, due to scaling issues and relatively high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBKKPnb0VNGu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s do the following:\n",
    "\n",
    "Omit the first 10 data points, which are on a different scale than the rest of the curve.\n",
    "Replace each point with an exponential moving average of the previous points, to obtain a smooth curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_MbicsjVNGu",
    "outputId": "ea1c86d5-297d-493a-d911-a97e7ecb8720",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8XXX9+PHXO3vvpE3StGnTlg7oohQ6oKyyhyAgoICAIooMxQE4cHxVUCk/URERFJEhAmXIxlIKhe490t20WW323rmf3x/n5ORmJ01ubm7yfj4e95F7z/ncez8nTe/7ftb7I8YYlFJKKQA/b1dAKaXU0KFBQSmllEODglJKKYcGBaWUUg4NCkoppRwaFJRSSjk0KCillHJoUFBKKeXQoKCUUsoR4O0K9FVCQoJJT0/3djWUUsqnbNy4scgYk9hTOZ8LCunp6WzYsMHb1VBKKZ8iIod7U067j5RSSjk0KCillHJoUFBKKeXQoKCUUsqhQUEppZRDg4JSSimHBgWllFIODQpKKTXEfLjrGEeKa7zy3hoUlFJqCKlrbObrz27gosc+9cr7a1BQSg15Tc0uln6wh5155d6uisftPVYJQFV9E80uM+jvr0FBKTXkfbKvkMc+2s/Fj62ipqHJ29UZUJ8fKCK3rNZ5vDOvwrm/52hrgBgsGhSUUkOaMYa/r8pyHr+346j3KjPAXC7D9X9by3lLV1Lb0MzW7LI2raGs4mp+9uZOTnzwfTYeLhmUOmlQUKqfXC7DD1/ZxuV//oy6xmZvV8dn/G/XMfbZXSWdaWx28eK6Izy/9gir9hfx88umkxoTynf/s5XZv/hgEGvqOQWV9QBUNzTzm3czufzPn/HejqOcmBoFwKGiav61xspjt2pf8aDUSYOCUv2UU1rLSxuy2Zpdxi3PrKeoqt7bVRryGppcfO3ZDSx59BOaml2dlnl61SHuX7adH7++g+kpUdw4fxwnpUYDUFrT2G1A8RVZxdXO/S3ZZQAUVTVwSnociZHBrM8qccYVnl510Blv8CQNCkr1U2FVnXP/8wPF3Pbs0E/tXlRVz7ee30h5TaNX3n+HWxfJjX9fx4HCqg5lXtuU69z/0UVTEREmjYpwji159BN++95uz1bUQ5qaXZTXNnLvf7Y6x7bltP5OpiVHkR4fxucHrNbBuPgwKuqaeHlDtsfrpkFBqX4qtLsA7jgrg1PSY9l0pGxQBwaPx18+PsA724/y8kbPf8h0ZmNWqXP/8wPF3PH8pg5l8strWTgxnj9dP5sFExMAiAkLalPmrW35nq2oB1TVN3Hqr5cz8+cfOAPM156SBsCoqGDuu3AKF89I5vbFGfiJ9Zynb5rLf74xn9sXZ3i8fhoUlOqnlqBw04J0bjvD+k87GM38/mgZ+xCRbss1uwz7Czp+i++vDYdLGBsX5jzefbSSa/662lmwVdvQTEVdEwsyErhkRopTbvHkthuH5ZXVdtn9NBSs3FvIPz/Pch7nlNZw6q/+R3F1g3PssetmMzMtBoCxcWHcvjiDsKAAzpk6ijfuWMTSa2YyMSmSeePjiI8I9nidNSgo1YM/r9jPi+uOdHm+oLIeP4H48GCmjI4EWqcSDkX//DyL59da11Ne09BlOZfLcMsz6zl36UrWHBy4QU5jDBuySpmbHsuDl04jNiwQgHWHSvj8QBEABZVWl9yoqJA2z52YFEHWQxeT9dDFPHTlSTS5DPnldQxVN/19HQ++uZOsImvs4K4XN1Pd0My4+NaAeNnMFCaPsv5uStt1550wOpIr54wZvArjg9txKjWYGppc/O79PQBcN29sp2X2F1QRHxGMv5+QGhNKWJD/kG4pPPjmTuf+4ZIaPj9QxIKMBIwxbVoOz609zMq9hQC8tD6b0ybED8j7ZxXXUFzdwNxxcVx/6lhuXjieusZmpv70PecD/liF1foaFdX1N+OWlkZ2aQ1pbq2OoaCusZmDha2DyK9szOHbZ09kc3YZtywcz7fPnsiuvApCg/wBnC8Tg9E91BMNCkp1Y2tOWbfn1xws5t0dR53/1H5+wti4MK/lrelJntsiKYA3tuTxxpY8Pv3BWZy7dCXXnzqWBy+dDsDHewrJSAznhNGRbD5S2tnL9cmhomoKK+s5bM+4mZse65wLCfQnISKYo05Q6Lyl4K4lELyyIYdRUSGEBPoTHx5ESKB/v+vaX3/6aD9/WrG/9fGK/RRU1mEMzEyLJi48iEWTEpzz4cEBZD10sTeq2oF2HynVjbe25jn3yzrpatmRa80Y+cXlJzrHxsWHcbhkaAaFzUesIPeDC05wAhnAG1tyqW9y8Y/PsiipbsDlMmzPLWfmmBhGRYVQVNV1N1NvXfPX1Vzz19W8sjGHqJAAJiZGtDmfHB3CUTsY5Jdbwau7oDAmNpTI4ACWbc7lnEdWsvChj/j+K9v6Xc/uZJfU8MxnhzCm8/QTFXWN1DU2899t1t/N/33hRC6YPhqA/2zIASCj3XUPNRoUlOrC+qwSXtqQTVSI1aA+3Mm3/8PFNUSFBDBvfJxzbFx8OEdKanB5IW8NWGMgM3/+AasPdBwH2JZTRpC/H19bNIFHvzTLGeD826eHnDJzfvkhv34nk8LKek5MjSYxMpiq+qZ+pZfILqlxBuTXHirh9MmJ+Pm1HeQeHRXitBQOFVUTFx5EdGhgl6/Z2SD5O9s9OxvpF2/t4mf/3cX7O9uuqn5zax4bD5cw42cfMOUn73G4uIbvn38CXzltHDctSG8TgMcnhHu0jv2lQUGpLjz64V7iwoJ44oaTAesDtb0jJTWMjW/bnz0uPoyGJhebs/vf5XI8VuwuoLy2kWWbcjqc25ZTztTkSIIC/JiaHMUbdyxkTGwo5bVtBzifWmUFifkZ8STaM16KKo+/tfD4xwcI9Bce//Icrj0ljV994cQOZZKjQ8gtq8XlMhwoqCYjsecPzyvmpLZ53OwynQbDgdLS/eYeRLfllHHXi5v54l9WO8eiQgL4yqnjAOt3+N49Z/DqNxfw3SWTCQ8e2r32GhTUsHagsIpP7MHSvnj84/18fqCYs6cmMX9CPJOSInjTrSsJ4LXNOazcW0h8eNvB0POnjyY5OoRfvzO4C6sy8yuobWh2Zj69vDGHO57fRLXbmol9BVWc4PatFWBBhjWAfPmsFB67bjZ/vn4OABMSw5maHEVipHV97ov0euNYRR2PfriXusZm3t6Wx2UzU7nopGQe+uKMDusNAGaNjaGqvokdeeUcLKpiQkLP3Sw/vngaz916aptj1/1tjTN7aSC5/243Hi7lOTv9xNudrJX40/VziA5r28o5eVwsd50zacDrNdA0KKhh7ZxHVnLj39f16Tm78ir47XvWjKOTUqMREa6Yk8r6rFLnQ+G5NYf5zkvWatTT3QYMARIigjlv2ij2HK3ssu95oO0+WsGFf/iU2b/8gEq3IPD29nxe22ytDC6vaaSoqr5Dn/Z3l5zAjDHRfP30CVw2M4WLZyTz5rcX8o+vngLQGhTs7p/CynoamnpeG/DkJwf5w/J9fO/lrVTUNTn5fLpy+iRrDcKyTbkUVTWQkdRzSyEowI9FkxL45PtnMd9tdtTrm3O7eVbf/HdrHvcv2862nDKaXIZfXXEikcEBPPjmTgoq6tqkqgBrEWNLoPVFGhTUsOXep9+XdA6vbba6XRIjgznDXix13SljCQrw4xV7BfCPX98BwJfmpnHLwvEdXmNiUgRV9U3O1EpPe3HtEQL8hPomF0H+fpx5Qusir/0FVdy/bDsz7SRyE5PaBoXR0SG8+e1FnGjnFQKYMSaGcfHWh3JLUNhfUEVNQxOLHv6IU371vx6T/63aZ605aFl1PKGHAdaEiGAmJUXwjL3Ya0FGQrfl3Y2ND+PXV57EpTNTGBUVzLpDA5dR9M4XN/PiuiO8a2dnvejEZN749kKaXYY3tuRxuLgGf3t8ZN74OL5//hQC/H33o3Vod24p1Q8Hi1q/wWWX1hAdFt1N6Va7j1ZyYmoUb915unMsNjyICQnhHCqqptFtBe2oqOAOA6YAGfYH7/6CKkZHWzNorn1yNX4ivPD1047retyVVjcQHRpIblktH+w6xr/WHObqk9O4aUE6UaEBhAcF8NKGbF7fnMvBouo2XWjtg0JPEiOCSYgI4vcf7GXph3txGahvcrEhq7TNtEp3W7LL2HOsklsXjedpe3xiQi8GWKenRLGvoIqkyGCmp3TfsmhvfEI4f7xuNve9uo13tud3WHdxPLLc/oae+TyLSUkRxIYHERseREp0CE+tOsixinpumj+O2WNjueDE0f16v6HAd8OZUj1wn5Of3W6KqDFW+obfvb/bmRPfYu+xSiYnte13BxgTG0ZOaa0zQyYqJICvdtJKADjBXqF654ubOPuRj8kuqWHNwRInwVlPXlp/hDN+u4JP93UcDymsrGf2Lz/kLysPcPYjH/PLt3bhMnDPkklMS4liTGwYseFB3L44g0mjItnfbiHd2D4u9BIRZ3DUfUJVd2s4XlqfTURwAN9ZMpnbzphASnQIqTGhPb5Xil3mjMmJx/2BPj0lioq6/rfSKuoaueHvawkOsD4mTxgVyR+vn+2cn5AY4bzHhMQIvjA7dUiskegvbSmoYavUbV3BkXZB4YV1R/jRa1YXUHx4MLcssj7cy2saOVZRz+TRHYNCWlwoq912yXr8yycTF95xwBQgPiKY6NBASmsaKa1p5LP9Rc653nyD/e/WfI6U1PDEygNOXztYayVu+5eVhfXVjTk0Nrd+So+K7Dinf2JiBP91GyA/YVTkcX3Y3jg/nV++tct5nBwd0ulsrBaZ+RWclBpNRHAAD1w0lfsvnNKr971yTior9xZyz7nHPyA73h6gPljU2krri8ZmF0s/3Mtzqw9T1dDEK7fPZ3pKNMEBfm2uIT7C+refMSaaL9kJ7YYDbSmoYatlmmVkcAAf7S5oc+65Na25jNwzmh4usboLOptLPiY2jOqGZq59cg0AqbHdf/M91W3twttu8+cr6trO9y+srGfNwWLqm5q57sk1PLZ8H6vsILL+UCm1Da199498sNdZgNY+E2tn3ViLJrUOeP70kmn862vzuq1zV25ZmE7mLy7gX7fO494lkzklPY6t2W33S84tq2VHbjnrs0o4UFDVJs11bwPRxKRI3r7rdMbEHn/aivH2VNZDRdU9lOzcf7fm8ZePD1BZ38SY2FBOHhdHSKB/h2sIsscNvnXmxGHRQmjhsaAgImkiskJEMkVkp4jc3UmZ74vIFvu2Q0SaRSSus9dTqq/K7MHlr58xgbWHSpwupOySGjLzK/jxxVOJCgmg2G1TnJauoeROvmG694mHBfkzpoeg8LurZvLy7fOJCQvk032tLYWj5XWU1TQ4LY4bnl7LtU+u4aPMAlYfLGbph3sBuHJ2Kg3NLrbbq6aNMby7ozW4tOza1Z1ZaVYqifjwIG5ZNJ6kTloTvSEihAb5c/qkRO48ZxIz02I4WlFHgVvX28KHPuKSP67i6idWU1nf1Oexi4GSHBVCcIBfm9xDfbHeLa335TNTuyz3/fNP4PbFGZw9Jem43meo8mRLoQm41xgzFTgNuENEprkXMMb8zhgzyxgzC7gfWGmMGZyNSNWwV1bTSERwABedZA3+fba/iLKaBs76/ceAlYY5ITKY7NJa/rxiP/VNzc74wuhO0issnNg6qLr1wfMI7GGGSXRYIKekxznBqUV+eS1n22kZwBrYBvjbpwedMpfOTOGecycDOBvQ5JTWUlTVwM8vm85bdy4C6DEw+fsJy+9dzPvfOaPbcn01c4w1aP/DV7dhjOl06m1fB4oHip+fMMXO13Tj39exPPNYn56/+Ugpp09K4NMfnNVtN1ZSVAj3XTiFoIDh1eHisTEFY0w+kG/frxSRTCAV2NXFU64DXvRUfdTIU1ZrzdDJSIwgMTKYVfuLKKttpMlliA4NZGJSBAnhwXy0u4CPdhcQHx7E0Yo6/P2k07z1QQF+/POWeZTXNvYYENwtvWYm67NK+d55kzntN8t5bPk+Sux8+ofd5rhvOtLaR/+VU8cyJjaU4AA/Dtj7GbS0GGaPjeHE1GhW3382YYEBnPXIx9x2xoQu398TuXZOTLWSuq3YU8jBomrCgtp2n3zltLHMGRvbxbM9b974OGfV8Sd7C3n8y3O46KTkHp9XWdfInmOVnD999JDLvDpYBiXEiUg6MBtY28X5MOAC4NUuzt8mIhtEZENhYd9Xp6qRqaymkdjwQESEi09K5u3t+Tz07m6mjI5k+b2LERESIlsHiourGzhaXk9SZLAz77y9xZMTuWxmSqfnunLlnDH85sqTiI8I5o/XzW7z4b/4dx+3KXvpzBSuOnkMc8bF4ucnTEiMcFoKW7PLCPATJ/d+cnQo0WGBbPrJkkFPuRwS6M+yby4ArJ3TMvMrnHMnpUbzf184qd/TQftjfrvFY9/qZGe3zmzJLsOYthlcRxqPBwURicD6sL/HGFPRRbFLgc+66joyxjxpjJlrjJmbmJjYWRGlOiiraSAm1PrQ/865k2np4bjtjAkk2C2ByODWVAT7C6o4WlHbbWbO/rrgxGR+fPHULs9/aW4av796ptMSyUgM50BhNS6X4a1t+SyYmDBkBjXHxYeRHB3CmoPFPO82cD8U5uqfOTmJp26c26dxjbe25fHIB3sRgVl2osCRyKNTUkUkECsgPG+MWdZN0WvRriPVT/uOVTIxKcL5hlpW20iyPe89OiyQf94yjze35LX5pr9gYjyf7iuktrGZ9VkllFQ3cPmsvrUE+uprp09gbFwYt/1rIwBx4UFce0oaY+PCOnzDzUiM4O3t+by4/gi5ZbXcd+EUj9atL0SEWWkxLM88Rl2ji7vOmcT188aSFOn5LSN74ucnnDttFHsLKp2UJT359gubAWvDm8iQrrOzDnceCwpi/c98Gsg0xiztplw0sBj4iqfqonxLXWMzz689wo3zx/XYd2+M4YmVB3llYzYHCqv56oJ0fnaZtUlMeU0jMW6plxdPTuywx+/ls1K5fFYqr27M4d6XrVxG503z/Ddd95QSm36ypMtyGUkRGAM/em0H89LjuLgX/eKD6aQx0U76h0tmJB/XugBP+sYZGRwsrOaVjTnUNzUTHNBzKyvDS7OmhgpPdh8tBG4AznabdnqRiNwuIre7lbsC+MAYc3zzx9Sw88TKA/zyrV29SmqWmV/Jw+/t5oA9/fCZz61NYowxlNU2EhPWu298V8xO5a5zJnH1yWPazDLylJZ8Qrcu6nxFdAv39NGPf2VOp2sRvKmlmyUlOoRJQ/DD1N9PnAHvkuoGlmce4yl7ltfqA8X85p1MoO2aj5NSe5cOZbjy5OyjVUCPf8HGmGeAZzxVD+V7WlYft8/x35mVnaTFfnNLLleePIZmlyG2kxTNnfHzE767ZHLfKtoPgf5+7P7lBc4CqK5kJEaQEh3CN8/McMZBhpL5E+J55uZTOCU9zqsDy91pWXVeXNXArf+0VoOfP3001/3NWoR45zmTnKnIV588hq/1EKiHO01zoYaclhTNOaW1PZSEdYeKmTwqgtlpsYxLCOPlDTms2l/EOVNHAXS7c5e39WbAOCTQn8/vP2cQanN8RIQzTxjai7da0lFsyS4jOjSQ8tpGnlh5wDn/9rY8csusoHDF7FSfznA6EDQoqCGnZTFXb9IUHCmpYVJSJA9fNcN6TmE1L2/MIdj+wO1sMxc1srQsRPzx6zsIsLvfXlzXOlvqh69ud+4neXDmma8Y2SFRDTmVdY1OS6H95iXtGWPIKa0lLa51VW/LeEDLbli9HVNQw1daXBj32l2DTS7DVxek09X22Z2lNxlptKWghoxml+HBN3YCVnrn/PK6LjOKGmP4y8oD1De52qw8vXxWCg3NLn7wyjYAYjUoKOCbZ2bwiJ1TavbYGBIjT2DNwWL8RCiurmdHbgUiDPn9kweDthTUkLFqfxHL7BlHCzLiaWhyOekg3Blj+L+3M5355ynRrS0FEeHqk8c4i5aiQ7X7SEGAvx/fWGylAhkTG8YdZ03kX7eeyj9vmcfr31rIZTNTePkb871cy6FBg4IaMtx3B2tZxJVf3nED9k1HSnl61SFm2tMhp7fb+1dEuOG0ccSHB2n3kXLcd8EU3rpzEXPGtl2tHODvx2PXzWZuuiZoBu0+UkPIukMlpMaE8r3zJzv7GeSV1bZZ6AWwYnch/n7Cs7fMIzI4oNO5+zfOH8d188b2KXGdGt5EpMPfkupI/8eoIcEYw8HCKpZMG8UVs8c4K2Pdt9RsKff+zqPMToshOjSwy8VcIjLsUhorNRi0paC8qqq+id++txtjoLqh2WkhJEYEMyoqmLWHStrsg/z+zqPsK6hi6TUzvVVlpYY1DQrKq9YeLObZ1Yedxy1BQUQ4Z+ooXt+cS25ZLakxoTQ1u/jpGzuZMjqyz+mrlVK9o+1r5VXF9uyiVDubqfu+vjecNg5/EX76+g4ACqvqKais54b540b8qlOlPEVbCsprNmSV8PGeAgDeu+d08svrSHabXjo1OYoLThzNR7sLOHfpStLjrfUIiUMwB5BSw4UGBeUVh4urueqJ1c7jiOAAZ0cxd1OTo3h5Yw7F1Q3st7el7GyrTKXUwNA2uPKKLdllbR53lWFzWiebvydE6II0pTxFg4Lyim055b0qd/K4WG47YwKXzGjdXEZbCkp5jgYFNWhW7ClgZ54VDLZkl/Vq/9xAfz8euGgqS6+Z5RwLDxoaexQrNRxpUFCD5uZ/rOfix1ZRXd/E1uwylkyz9jzozSIz9zJDdTMXpYYDHWhWg6LabbvD1zbn0uQyLMiIZ8m0Ub2eTfTARVMoquqYIE8pNXA0KKhB0bLFJlibnYQF+XNKelyvdh9rcdsZGZ6omlLKjXYfqUFxuNgKCmdPsbZunD02pk8BQSk1OLSloDyusdnFw+/tBuCRq2fy2uZczpoytPf1VWqk0qCgPO5wcTWHiqq5+uQxxIYHccui8T0/SSnlFdp9pDwur8zaKOfquWlerolSqicaFJRHuFyGN7bk0tjs4qi9e5puiq7U0KfdR8oj/rstj7v/vYVjFXXUNrgASIrSlchKDXUaFJRH5No7ph0tr6espoGEiCCCA3S2kVJDnQYF5RGl9j4Jn+wrZH9BFTPG6N64SvkCHVNQHpFlr0toSXf9i8tP9GZ1lFK9pEFBecSR4tYVzIsmJjArLcaLtVFK9ZYGBeURJTVW91FwgB+XzdL9lJXyFTqmoAacMYbymkZuX5zBfRdO8XZ1lFJ9oC0FNeBqG5tpaHYRExbo7aoopfqoy6AgIj9wu391u3O/9mSllG8rq2kEICZUg4JSvqa7lsK1bvfvb3fuAg/URQ0TTlDQloJSPqe7oCBd3O/ssVIUVNbx49e3c6zCSmsRHRrk5Roppfqqu4Fm08X9zh4rxRMfH+S5NUecvRO0paCU7+kuKMwUkQqsVkGofR/7sWY2Ux2EBFoNzy3ZZYAGBaV8UZfdR8YYf2NMlDEm0hgTYN9vedzj/3YRSRORFSKSKSI7ReTuLsqdKSJb7DIr+3Mxyrsq65qcnyIQG6bdR0r5mj6tUxCRcOALwPXGmIt7KN4E3GuM2SQikcBGEfnQGLPL7fVigMeBC4wxR0REt+PyYS1jCQBXzE7V7TaV8kE9BgURCQIuAq7HmnX0KvBET88zxuQD+fb9ShHJBFKBXW7FrgeWGWOO2OUK+noBaug4VlnP6ZMSuOOsiZrWQikf1d06hSUi8nfgEHAV8C+gxBhzszHmv315ExFJB2YDa9udmgzEisjHIrJRRG7sy+uqoaWgoo7RUSGcNiFeWwlK+ajuWgrvA58Ci4wxhwBE5A99fQMRicBqXdxjjKlodzoAOBk4BwgFVovIGmPM3navcRtwG8DYsWP7WgU1CMpqGjhaUce4+DBvV0Up1Q/drVM4GVgD/E9EPhSRW4E+ff0TkUCsgPC8MWZZJ0VygPeMMdXGmCLgE2Bm+0LGmCeNMXONMXMTExP7UgU1SNZnlWIMnJIe5+2qKKX6obvZR5uNMT80xmQAP8Pq/gkSkXftb+7dEhEBngYyjTFLuyj2BnC6iASISBhwKpDZ14tQ3rfxcClB/n7M1LEEpXxar2YfGWM+Az4TkbuAJVgpMJ7s4WkLgRuA7SKyxT72ADDWfs0njDGZIvIesA1wAU8ZY3b0/TKUt5XVNBAbHqhjCUr5uC6DgojM6eJUIfDHnl7YGLOKXqTDMMb8DvhdT+XU0Fbb2EyoBgSlfF53LYUNwE6sIABtP+ANcLanKqV8T01Ds7YSlBoGugsK9wJfBGqBfwOvGWOqBqVWyufUNTYTGqRBQSlf191A86PGmEXAt4E0YLmI/EdEZg1a7ZTPqG3Q7iOlhoMed16z1yi8AXwAzMNacKZUG7WNzYRpS0Epn9fdQPMErFlGlwPZWF1IvzLG1HX1HDVy1TbqmIJSw0F3Ywr7saaKvgFUYE0l/Za1/AC6WXugRiDtPlJqeOguKPyC1s10IgahLsqH1epAs1LDQpdBwRjzs0Gsh/Jx2lJQanjocaBZqZ64XIb6Jpe2FJQaBjQoqH6ra2oG0JaCUsOABgXVbzUNdlDQloJSPq83O68FY61sTncvb4z5heeqpXxJrR0UdEqqUr6vN1lS3wDKgY1AvWero3xRXaMVFHTxmlK+rzdBYYwx5gKP10T5rPLaRgAignuViV0pNYT1ZkzhcxE5yeM1UT4rt6wWgDGxoV6uiVKqv3rz1W4R8FUROYTVfSSAMcbM8GjNlM/IKbWCQmqM7s+slK/rTVC40OO1UD4tp7SGhIggnX2k1DDQmyyph4EY4FL7FmMfUwqwWgqpsdpKUGo46DEoiMjdwPNAkn17TkTu9HTFlO/ILa1lTIyOJyg1HPSm++hW4FRjTDWAiDwMrKYX+zSr4c8YQ155LWdPSfJ2VZRSA6A3s48EaHZ73Ezb/ZrVCFZe20hdo4tkbSkoNSz0pqXwD2CtiLxmP/4C8LTnqqR8SV6ZtedScnSIl2uilBoIPQYFY8xSEfkYa2qqADcbYzZ7umLKNxytsKajalBQanjobjvOKGNMhYjEAVn2reVcnDGmxPPVU97S2Oxiz9FKpqdE0bLbXmdaWgop2n2k1LBqxSAjAAAZCklEQVTQ3ZjCC/bPjcAGt1vLYzWMvbM9n0v+uIoHXtvuHGtsdmGMaVPuaHkdAX5CQkTwYFdRKeUBXQYFY8wl9s/xxpgJbrfxxpgJg1dF5Q0FFVbuw/9lFgBQ39TMqb9ezqubcgFodhk+219EXnkto6JC8PfTuQdKDQe9WaewvDfH1PDSskdCYWU9335hE5/tL6KkuoHP9hcB8If/7eXLT61l2aZcRut4glLDRndjCiFAGJAgIrG0TkONAlIGoW7Ki2oampz7b23Ld4LBjtxyAKfFADrIrNRw0t3so28A92AFgI20BoUK4M8erpfyspaWQovSGis99oHCKg4XVzuZUQEiQzRltlLDRXdjCn8wxowHvuc2ljDeGDPTGPOnQayj8pDM/ArO+v3HZBVVdzhX3dBEakwoL3z9VJ66ca5z3GXg2dVW6qv7LpwCwMwxMYNTYaWUx/VmncIfReREYBoQ4nb8WU9WTHne/cu2c6iomuW7Czh3ahLff3kbiVHB/O6qGdQ2NBMe7M+CjATqGpsJDvCjodmFMfD0qkNEBAdwy8LxXDM3jZjQQG9filJqgPRmj+YHgTOxgsI7WKm0VwEaFHxYdX0TO/Os8YEDhVUs25TDzrwKALKKqgkJ9Cc0yPrzCAn0Z9HEBHLLatl9tBKAp26aS1CAH3EBQd65AKWUR/SmM/gqYCaw2Rhzs4iMAp7ybLWUp326r4jGZmvNwQtrjwDwu6tmUFbTyK/eyQRgQUa8U/5hu/XwysYcwoL8OW1CfMcXVUr5vN4EhVpjjEtEmkQkCigAdJ2Cj9uRW46/n/DNxRk8v/YwNy1I54rZqeSX1zlBIcxt05yWxWnfWTLZK/VVSg2O3gSFDSISA/wNaxZSFbDOo7XykILKOipqG5mYFOntqnhdcXUDsWGBfO/8E7j3vMlOKoukqNaVyWFBOqtIqZGmNzuvfcsYU2aMeQJYAtxkjLnZ81UbeD96bQfnLv2ElXsLvV0Vryupricu3BoPcM9tFBzgT0yYNXAcpttrKjXidLd4bU5354wxmzxTJc/JLqkBYGNWCYsnJ3q5Nt5VUt3gBIX2RkWGUFbTqC0FpUag7v7XP2L/DAHmAluxFrDNANZipdLukoikYc1QGg24gCeNMX9oV+ZM4A3gkH1omTHmF327hN5ryeWWa2f2HMmKqxuYOjqq03NJUcHsOVZJeLC2FJQaabpbvHaWMeYs4DAwxxgz1xhzMjAb2N+L124C7jXGTAVOA+4QkWmdlPvUGDPLvnksIABU1VupG/LLa3soOfyVVjcQG975+oI5Y2MBmJrcedBQSg1fvekfmGKMcfInG2N2iMisnp5kjMkH8u37lSKSCaQCu463sv1VWWelasgrG9lBodllKKttJC6883TX31kymXvOndTtPgpKqeGpN3s0Z4rIUyJypogsFpG/AZl9eRMRScdqYazt5PR8EdkqIu+KyPS+vG5fGGOclkJeWR0ul+nhGcNXaU0DxkB8F2MKgAYEpUao3gSFm4GdwN1YCfJ22cd6RUQigFeBe4wxFe1ObwLGGWNmAn8EXu/iNW4TkQ0isqGw8PhmDtU1unAZGBcfRkOzq01Ct5GmtLoBgNhugoJSamTqzZTUOmPMo8aYK+zbo8aYXo3UikggVkB43hizrJPXrjDGVNn33wECRSShk3JP2mMacxMTj2/WUGW91XU0Lz0OwEnpMBIV20Ghu5aCUmpk6jIoiMh/7J/bRWRb+1tPLyxW/8PTQKYxZmkXZUbb5RCReXZ9io/nQnpSVWd1HZ08LhY/gV35Iy8oGGP4zktbWLYpB6DLKalKqZGru4Hmu+2flxznay8EbgC2i8gW+9gDwFgAezHcVcA3RaQJqAWuNe03AR4gLeMJiZHBTEiMYNcIbClk5lfy2ubWzXG0paCUaq/LoGDPHsIYc/h4XtgYs4rWjXm6KvMnYFD2ZmhpKUQEBzAtOYqNh0sH422HlBV7Cto8jgnToKCUaqu77qNKEano5FYpIj73NbvSbilEhAQwPSWK3LJaZ8B1pGjfZRYU0Jt5BkqpkaS7xWuRxpioTm6RxhifW9UUHx7E+dNHkRARzImp0QA89O5uL9dqcOWV1ZIaE+rtaiilhrBef1UUkSQRGdty82SlPGFuehx/vWEuo6JCmD8hnotPSuY/G7M5Wj5yUl7kltaycKLug6CU6lqPQUFELhORfVj5iVYCWcC7Hq6XR/n5Cd89bzLGwLs78r1dnUFR39RMQWU9KTGhrLn/HD6/72xvV0kpNQT1pqXwS6zcRXuNMeOBc4DPPFqrQZCRGEFCRBB7j1V6uyqD4lh5PQApMaGMjg4hRbuRlFKd6E1QaDTGFAN+IuJnjFkB9Jj7yBekxISSN0Iyph4qrgYgLTbMyzVRSg1lvUmIV2anqvgEeF5ECrAyoPq85OgQDhZWe7sag2JbdhkA01N9bo6AUmoQ9aalcDnWwrLvAO8BB4BLPVmpwZISE8q+gire3jb8xxW25ZYzISGcqJDO02UrpRR0v07hTyKywBhTbYxpNsY0GWP+aYx5zO5O8nnh9s5id7zgc5vI9Umzy7DxcCmz0mK8XRWl1BDXXUthH/CIiGSJyMO92UPB15w9Ncm539js8mJNPGtLdhkl1Q2cOSWp58JKqRGtu8VrfzDGzAcWAyXAP0QkU0R+KiKTB62GHjRnbCy/v3omAEfs/ZuHo+WZx/D3ExZPGtn7Uiuletab1NmHjTEPG2NmA9cDV9DHTXaGsozEcIBhO+BcWdfIfzZkc0p6LNFhOp6glOpebxavBYrIpSLyPNaitb3AFz1es0EyPsEKCoeLh19Q2J5Tzqm/Xk5RVQNLpo32dnWUUj6gyympIrIEuA64GFgH/Bu4zRgzrD49o0MDCQvyH5brFf7x2SGaXYanb5rL6dp1pJTqhe7WKTwAvAB8zxhTMkj1GXQiYi9iG37bc3646xiXz0rhnKmjvF0VpZSP6G4/hbMGsyLelBwdQl758AoKNQ1NVNY3MSExwttVUUr5EE2oD6QOw3QXxVW6D7NSqu80KGCtbC6qqqeusdnbVRkQOaU17DlqJfqLj9CgoJTqvd7kPhr2WjKGHi2vI92ejeTLFj28wrkfHx7sxZoopXyNthSAlJgQgGE52KwtBaVUX2hQAGeLyrxhuAubthSUUn2hQQEYHT18WgruOZzCgvwJDfL3Ym2UUr5GxxSA4AB/EiKCfTIo/G/XMSJCAjhtgrX3clWdtdXFlbNTmZ+h+zErpfpGWwq21JgQcn0gKFTUNbbZQvRrz27g2ifXsPFwCQt+s9zZYW3BxASunpvmrWoqpXyUBgVbSkwon+4r4uZ/rPN2Vbr1zec2ct6jn3SYPvvAsh3kldfxyd5CAKJCtBGolOo7DQq2lmmpK/YU4nIZL9ema5/tt/Y3uvWf62loah0/2GO3Hg4XWynAI3WHNaXUcdCgYEu2B5sBiqrqvViT7sXY6a8/21/MhqyOKalaFq1FhWpLQSnVdxoUbM1urYPs0qE5tlDb0ExZTSMi1uN9BVUdyrSMN+hezEqp46FBwfaF2akE+Vu/jpzSobkLW7Zdr4e/OAOA3Ucr25w/YVQkTXZwiwrVoKCU6jsNCrZRUSFseXAJADlDtKVQWm0luUuJDiUhIog9Ryucc/PS4zh1QhwAX1s0nmgNCkqp46Adz27CggKICgngWIX3VjYbY/jFW7u4dGYKc8bGtjlXXtsIWOMKKTGh7D1mdR/dcNo4fnTxVIID/LjvwimEBek/q1Lq+GhLoZ2kqBAKK7030Fzf5OIfn2Xx6sacDufK7KAQHRrI+IRwquqthWpfmJ1KSKA/IqIBQSnVLxoU2kmMCKbAi0GhpsFaf+C+QK1FeU1rS+Gecyc7x3VNglJqoGhQaCcpKtirLYVae1HanqOVGNN2vURZbQP+fkJEcADj3VJ8R2hQUEoNEA0K7VgthboOH8iDpbbB6hKqqGtic3ZZm4V05bWNRIcGIvac1De/vZDLZqaQGKGZUJVSA0ODQjtJUcHUNbp4bu0Rr7x/S/cRwJWPf84L61rrUVbTSIzbrKIZY2J47LrZBPjrP6NSamDop0k7M8bEAPDkJwcAOFRUPajv7x4UALbnlAOwK6+Ct7blU++W2kIppQaaBoV2TpsQz62LxlNU2cDb2/I56/cfs2JPwaC9f227oBAWbO2H8PqWXABnLYJSSnmCx4KCiKSJyAoRyRSRnSJydzdlTxGRZhG5ylP16Yv0+DBqG5u544VNALy3/Sh/++TgoIwztAw0nzzOWqPQMhNqR245J6ZGOauZlVLKEzzZUmgC7jXGTAVOA+4QkWntC4mIP/Aw8L4H69InY+PD2zx+aUM2v3on08lA6kkt3UePXjOLU8fHUVBhDXrvyC1nxpgYAnX8QCnlQR77hDHG5BtjNtn3K4FMILWToncCrwKD10fTg0lJEZ0ezx+EPZxbZh+FBvkTHRrI+qxSfvLGDirqmpg6OtLj76+UGtkG5WuniKQDs4G17Y6nAlcAT/Tw/NtEZIOIbCgsLPRUNR0pMaF8+oOzuOucSW2OD0b6i5aWQliQv5MN9bk11gykCYmdByullBooHg8KIhKB1RK4xxhT0e70/wN+aIxp7vjMVsaYJ40xc40xcxMTEz1V1TbS4sL47pLJ7Pj5+Xz8vTMBODqIQSEk0J8fXzyNAD9xzk1IDO/qaUopNSA8GhREJBArIDxvjFnWSZG5wL9FJAu4CnhcRL7gyTr1VURwAOkJ4UQEB3B0ELqP6hqbCQ7ww99PSIsL486zrdaKCIyKDOnh2Uop1T8ey48g1rLbp4FMY8zSzsoYY8a7lX8GeMsY87qn6tQfo6NDyC3zfErtmoZmwoL8ncfpCWEATEgIx8+t1aCUUp7gyaQ5C4EbgO0issU+9gAwFsAY0+04wlBz6vg4Xlx3hJ155UxPifbY+1TWNbbJdHrm5CRuXTSe2xdneOw9lVKqhceCgjFmFdDrr7bGmK96qi4D4d7zTuD5tUdYubfQo0FhR14FJ7jNMooOC+Qnl3SYyauUUh6hk957KS48iNSYUDLzO6a0HihlNQ3sL6hyFq4ppdRg06DQB1OTo8jMbz+BauBszi4D6LDjmlJKDRYNCn0wLSWKg4VV1DV2O4P2uDQ0uXhnWz7+fsLMNM91TymlVHc0KPTBtORIXMbaAGegPbXqIC9vzCEiOEC31FRKeY0GhT6Ylmx9g1+5d+BXVW8+YnUdfetMnWWklPIeDQp9MCY2FIClH+5lXyd7KPdHeW0j89Lj+IZOPVVKeZEGhT7w8xN+efl0APYVVA3oa+eU1DhBRymlvEWDQh9dOjMFgLwBXN3c0OTiaEWdBgWllNdpUOij6NBAQgP9ySvrex6kgoo66ps6zlw6Wl6Hy8CYuLCBqKJSSh03DQp9JCIkx4SQX963lkJTs4t5v17OXS9u7nAup9TavEdbCkopb9O5j8chJTq0z91HB4uqAXh/5zHnWEOTi5/9dyflNY0ApMVqS0Ep5V0aFI7D1ORI/vn5YSrrGokMCeyxfFFVPec9+onzuLahmUaXi288u5HVB4ud46OjNTW2Usq7tPvoOJw3fTQNza5er1doWYPQ4slPDrIhq4TVB4uZNz7OOa77LyulvE0/hY7DnLGxRIcGsmpfUZdl/r3uCD95fQcbsko4ao8/PHDRFM6YnMgbW3LJKrLGER7/8hxGRQUzMy1mUOqulFLd0e6j4+DvJ8wbH8cat64fd5uPlHLfsu0AvLQ+m5PHxRLk78fXFk2gsq6Jz/YXsb+wivAgf+LDg/jkB2fhJ7qBjlLK+7SlcJxOHR9HVnENBZ3s2/z65lzCgvz5/L6zCQrwY/XBYpJjQvDzEyYmRdDsMqzcU8jY+HBEhOAAf+06UkoNCfpJdJxm2d09m46UUVnX2OZcfnkdabFhpMSEcsbkBABSY6zpphOTIgDILatlQkL4INZYKaV6pt1Hx2lqchQicPtzGwE4+OuLnD2Uj1XWkxQVDMD188ZxsLCay2dZK6FPGBXJdfPSiAkL4tpT0rxTeaWU6oIGheMUHhzAhIRwDhRa6w8OFVeTkWi1Agor6piYaLUQFk1K4L17znCeF+Dvx2+unDH4FVZKqV7Q7qN+GBff2v2zI7ccAJfLUFjV2lJQSilfoi2FfnBfbPbb9/YgIizMiKex2TAqUoOCUsr3aEuhH1LsoBAe5M+xijp++sYO8sut2UhJUbo6WSnlezQo9ENChNUaOH/6aB65ZiZlNY18tLsAgFHafaSU8kEaFPqhZW1Bk8uwaKI1sPz6llwAkiK1paCU8j0aFPrhtIx4AK6dl0Z8RDBpcaEctGcjJeqYglLKB+lAcz+kxoSS9dDFzuOTUqPJLqklOjSQkEB/L9ZMKaWOj7YUBtCMMdYq54Yml5dropRSx0eDwgBqWbVc29hxy02llPIF2n00gJKjQ3n4iyfpILNSymdpUBhgXzplrLeroJRSx027j5RSSjk0KCillHJoUFBKKeXQoKCUUsqhQUEppZRDg4JSSimHBgWllFIODQpKKaUcYozxdh36REQKgcPH+fQEoGgAq+Mr9LpHnpF67XrdXRtnjEns6YV8Lij0h4hsMMbM9XY9Bpte98gzUq9dr7v/tPtIKaWUQ4OCUkopx0gLCk96uwJeotc98ozUa9fr7qcRNaaglFKqeyOtpaCUUqobIyIoiMgFIrJHRPaLyH3ers9AE5G/i0iBiOxwOxYnIh+KyD77Z6x9XETkMft3sU1E5niv5v0jImkiskJEMkVkp4jcbR8f1tcuIiEisk5EttrX/XP7+HgRWWtf90siEmQfD7Yf77fPp3uz/v0lIv4isllE3rIfD/vrFpEsEdkuIltEZIN9zCN/58M+KIiIP/Bn4EJgGnCdiEzzbq0G3DPABe2O3QcsN8ZMApbbj8H6PUyyb7cBfxmkOnpCE3CvMWYqcBpwh/1vO9yvvR442xgzE5gFXCAipwEPA4/a110K3GqXvxUoNcZMBB61y/myu4FMt8cj5brPMsbMcpt66pm/c2PMsL4B84H33R7fD9zv7Xp54DrTgR1uj/cAyfb9ZGCPff+vwHWdlfP1G/AGsGQkXTsQBmwCTsVavBRgH3f+7oH3gfn2/QC7nHi77sd5vWPsD8CzgbcAGSHXnQUktDvmkb/zYd9SAFKBbLfHOfax4W6UMSYfwP6ZZB8flr8Pu2tgNrCWEXDtdhfKFqAA+BA4AJQZY5rsIu7X5ly3fb4ciB/cGg+Y/wf8AHDZj+MZGddtgA9EZKOI3GYf88jf+UjYo1k6OTaSp1wNu9+HiEQArwL3GGMqRDq7RKtoJ8d88tqNMc3ALBGJAV4DpnZWzP45LK5bRC4BCowxG0XkzJbDnRQdVtdtW2iMyRORJOBDEdndTdl+XfdIaCnkAGluj8cAeV6qy2A6JiLJAPbPAvv4sPp9iEggVkB43hizzD48Iq4dwBhTBnyMNaYSIyItX/Tcr825bvt8NFAyuDUdEAuBy0QkC/g3VhfS/2P4XzfGmDz7ZwHWl4B5eOjvfCQEhfXAJHuGQhBwLfCml+s0GN4EbrLv34TV395y/EZ7hsJpQHlLE9TXiNUkeBrINMYsdTs1rK9dRBLtFgIiEgqcizXwugK4yi7W/rpbfh9XAR8Zu7PZlxhj7jfGjDHGpGP9P/7IGPNlhvl1i0i4iES23AfOA3bgqb9zbw+gDNIgzUXAXqx+1x95uz4euL4XgXygEetbwq1YfafLgX32zzi7rGDNxjoAbAfmerv+/bjuRVjN4m3AFvt20XC/dmAGsNm+7h3AT+3jE4B1wH7gZSDYPh5iP95vn5/g7WsYgN/BmcBbI+G67evbat92tnyGeervXFc0K6WUcoyE7iOllFK9pEFBKaWUQ4OCUkophwYFpZRSDg0KSimlHBoUlLKJSLOdhbLlNmAZdUUkXdyy2Co1VI2ENBdK9VatMWaWtyuhlDdpS0GpHti57B+29zBYJyIT7ePjRGS5nbN+uYiMtY+PEpHX7P0OtorIAvul/EXkb/YeCB/Yq5ERkbtEZJf9Ov/20mUqBWhQUMpdaLvuoy+5naswxswD/oSVbwf7/rPGmBnA88Bj9vHHgJXG2u9gDtYqVLDy2//ZGDMdKAO+aB+/D5htv87tnro4pXpDVzQrZRORKmNMRCfHs7A2tTloJ+A7aoyJF5EirDz1jfbxfGNMgogUAmOMMfVur5EOfGisDVEQkR8CgcaY/xOR94Aq4HXgdWNMlYcvVakuaUtBqd4xXdzvqkxn6t3uN9M6pncxVq6ak4GNbhk/lRp0GhSU6p0vuf1cbd//HCtbJ8CXgVX2/eXAN8HZDCeqqxcVET8gzRizAmvzmBigQ2tFqcGi30iUahVq72bW4j1jTMu01GARWYv1Reo6+9hdwN9F5PtAIXCzffxu4EkRuRWrRfBNrCy2nfEHnhORaKzslo8aa48EpbxCxxSU6oE9pjDXGFPk7boo5WnafaSUUsqhLQWllFIObSkopZRyaFBQSinl0KCglFLKoUFBKaWUQ4OCUkophwYFpZRSjv8Phb5Se3qCOA8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZobjJMFVNGw",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "According to this plot, validation MAE stops improving significantly after 80 epochs. Past that point, you start overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbFdJweGVNGx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you’re finished tuning other parameters of the model (in addition to the number of epochs, you could also adjust the size of the hidden layers), you can train a final production model on all of the training data, with the best parameters, and then look at its performance on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FbeNoB34VNGx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_b5eA9gLVNG4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here’s the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 686,
     "status": "error",
     "timestamp": 1541848725975,
     "user": {
      "displayName": "Лингвисты 4 курс 15ФПЛ",
      "photoUrl": "",
      "userId": "07722014738513937459"
     },
     "user_tz": -120
    },
    "id": "2tw3TpG9VNG8",
    "outputId": "f116bf95-5545-417d-9528-d23bb64f19e5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0971a0626328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_mae_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_mae_score' is not defined"
     ]
    }
   ],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwOOqnFeVNG_",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You’re still off by about \\$2,550.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bte08ubnVNHA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapping up\n",
    "\n",
    "* Regression is done using different loss functions than what we used for classification. Mean squared error (MSE) is a loss function commonly used for regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFZXXbacVNHA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally, the concept of accuracy doesn’t apply for regression. A common regression metric is mean absolute error (MAE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxfRjf54VNHB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuA0pHKjVNHC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When there is little data available, using K-fold validation is a great way to reliably evaluate a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHNDlVxDVNHH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When little training data is available, it’s preferable to use a small network with few hidden layers (typically only one or two), in order to avoid severe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LvrXIvMDVNHI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture2(NN).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
